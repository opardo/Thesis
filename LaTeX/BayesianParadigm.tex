\chapter[Paradigma bayesiano]{Paradigma bayesiano\raisebox{.3\baselineskip}{\normalsize\footnotemark}}\footnotetext{Las ideas de este cap\'itulo son retomadas de \cite{Denison_BayesMethods}.}


\section{Axiomas}
Esta tesis da como aceptados los axiomas de la Estad\'istica Bayesiana, mismos que pueden ser encontrados, por ejemplo, en \cite{Fishburn_Axioms}. Por lo tanto, entiende a dicho paradigma como el coherente para hacer estad\'istica, cuando una toma de decisi\'on con incertidumbre es el objetivo final del estudio. 

\section{Inferencia de variables aleatorias}

Un problema clásico de la estad\'istica es el de hacer predicci\'on, utilizando la informaci\'on de los datos que ya han sido observados. Por ejemplo, es posible pensar que ya se tiene el conjunto de $n$ datos observados $\{y_1, ..., y_n\}$ y se desea hacer predicci\'on acerca del valor del dato $y_{n+1}$, que a\'un no ha sido observado. Para esto, se podr\'ia usar la probabilidad condicional
\begin{equation*}
    \mathbb{P}(y_{n+1}|y_1,...,y_n) =
    \frac{\mathbb{P}(y_{n+1} \cap \{y_1, ..., y_n\})}{\mathbb{P}(y_1, ..., y_n)} =
    \frac{\mathbb{P}(y_1, ..., y_n,y_{n+1})}{\mathbb{P}(y_1, ..., y_n)},
\end{equation*}
pero esto requerir\'ia conocer la funci\'on conjunta, misma que puede ser compleja por la estructura de dependencia de los datos.\footnote{Cabe resaltar que en este trabajo se usar\'a la notaci\'on $\mathbb{P}$ como una forma general de definir una medida de probabilidad, independientemente de los asociados detalles te\'oricos sobre an\'alisis y medibilidad.}

Este problema puede ser abordado mediante el Teorema de representaci\'on general de de Finetti. Para ello, antes se dar\'a una definici\'on.

\begin{defin*}
    Sea $(y_1,...,y_n)$, una secuencia de $n$ variables aleatorias, cuya distribuci\'on de probabilidad conjunta est\'a dada por $\mathbb{P}(y_1,...,y_n)$. Sea $\psi$ una funci\'on biyectiva, que va de $\{1,...,n\} \rightarrow \{1,...,n\}$, es decir, una funci\'on que crea una permutaci\'on del conjunto $\{1,...,n\}$.  
    Entonces, se dice que $(y_1,...,y_n)$ es una \textbf{secuencia aleatoria infinitamente intercambiable} si se cumple que 
    \begin{equation*}
        \mathbb{P}(y_1,...,y_n) = \mathbb{P}(y_{\psi(1)},...,y_{\psi(n)}),
    \end{equation*}
    para cualquier permutaci\'on $\psi$.
\end{defin*}

En pocas palabras, una secuencia $(y_1,...,y_n)$ se considerar\'a infinitamente intercambiable si el orden en que se etiquetan las variables no afecta su distribuci\'on conjunta. Es importante hacer notar que la com\'unmente usada independencia implica intercambiabilidad, pero lo contrario no se cumple. Es decir, la intercambiabilidad es un supuesto menos r\'igido que la independencia.

Dicho esto, es momento de plantear el \textbf{\textit{Teorema de represesentaci\'on general de de Finetti}}.\footnote{Una demostraci\'on de este teorema puede ser encontrada en \cite{Schervish_TheoryStats}.}

\begin{theorem*}
    Sea $(y_1, ...,y_n)$ una secuencia aleatoria infinitamente intercambiable de valores reales. Entonces existe una distribuci\'on de probabilidad $F$ sobre $\mathcal{F}$, el espacio de todas las distribuciones, de forma que la probabilidad conjunta de $(y_1, ...,y_n)$ se puede expresar como
    \begin{equation*}
        \mathbb{P}(y_1, ...,y_n) =
        \int_{\mathcal{F}}\left[\prod_{k=1}^n \mathbb{P}(y_k|G)\right]dF(G),
    \end{equation*}
    con
    \begin{equation*}
        F(G) = \lim_{n \to \infty} F(G_n),
    \end{equation*}
    donde $F(G_n)$ es una funci\'on de distribuci\'on evaluada en la funci\'on de distribuci\'on emp\'irica definida por
    \begin{equation*}
        G_n(y) = \frac{1}{n} \sum_{i=1}^n I(y_i \leq y).
    \end{equation*}
    En otras palabras, el Teorema de de Finetti dice que $\{y_1, ...,y_n\}$ es un conjunto de variables aleatorias condicionalmente independientes, dada cierta distribuci\'on $G$. A su vez dicha $G$ es aleatoria y sigue una distribuci\'on $F(G)$.
\end{theorem*}

Cabe hacer notar que dicho teorema plantea la distribuci\'on conjunta de $(y_1, ...,y_n)$ como una mezcla de verosimilitudes condicionalmente independientes en $G$, donde el peso asociada a cada una depende de $F(G)$. Por lo tanto, $F(G)$ expresa la creencia o conocimiento acerca de cu\'an probable es que $G$ sea id\'oneo para explicar el fen\'omeno, a\'un sin observar los datos.

Un subconjunto del espacio de todas las distribuciones $\mathcal{F}$ es el espacio de las distribuciones param\'etricas, es decir, aquellas que pueden ser descritas en su totalidad \'unicamente señalando el valor de un vector de par\'ametros de tamaño finito $\theta$, mismo que puede tomar valores en todo el soporte $\Theta$. Por lo tanto, si se hace el supuesto adicional que la distribuci\'on marginal de $y_i$ es param\'etrica, con vector de par\'ametros desconocido, se obtiene como corolario del Teorema de de Finetti que
\begin{equation*}
    \mathbb{P}(y_1, ...,y_n) =
    \int_{\Theta}\left[\prod_{k=1}^n \mathbb{P}(y_k|\theta)\right]\mathbb{P}(\theta)d\theta.
\end{equation*}

Siguiendo el razonamiento anterior, $\mathbb{P}(\theta)$ indica la probabilidad de que $\theta$ sea el vector de par\'ametros id\'oneo para explicar el fen\'omeno, antes de observar cualquier dato.

La intuci\'on detr\'as de este resultado es que, al igual que en otros paradigmas, se supone a $\theta$ como constante, pero desconocido, y la tarea es estimarlo. Una particularidad del paradigma bayesiano es expresar la incertidumbre que tiene el modelador acerca del valor verdadero mediante la asignaci\'on de una distribuci\'on a $\theta$, sujeta la informaci\'on inicial o conocimiento previo que se tenga del fen\'omeno $(H)$. Es decir, $\mathbb{P}(\theta|H)$. Como una simplificaci\'on de la notaci\'on, en la literatura normalmente se escribe como $\mathbb{P}(\theta) = \mathbb{P}(\theta|H)$ y se conoce como la \textit{probabilidad inicial} del par\'ametro.

Regresando al problema inicial, y bajo los supuestos reci\'en mencionados, es posible escribir
\begin{equation*}
    \mathbb{P}(y_{n+1} | y_1,...,y_n) =
    \int_{\Theta} 
    \mathbb{P}(y_{n+1}|\theta) \mathbb{P}(\theta|y_1,...y_n)d\theta,
\end{equation*}
donde a su vez, usando el \textbf{Teorema de Bayes}, se obtiene que
\begin{equation*}
\begin{aligned}
    \mathbb{P}(\theta|y_1,...,y_n) &=
    \frac{\mathbb{P}(y_1,...,y_n|\theta)\times\mathbb{P}(\theta)}
    {\mathbb{P}(y_1,...,y_n)},
\end{aligned}
\end{equation*}
que en el paradigma bayesiano se conoce como la \textit{probabilidad posterior} del par\'ametro. 

Se puede observar que el denominador no depende de $\theta$, y en realidad es una constante dada por la representaci\'on del teorema de de Finetti de la probabilidad conjunta, por lo que normalmente la probabilidad condicional del vector de par\'ametros no se expresa como una igualdad, sino con la proporcionalidad
\begin{equation*}
    \mathbb{P}(\theta|y_1,...,y_n) 
    \propto 
    \mathbb{P}(y_1,...y_n|\theta) \times \mathbb{P}(\theta),
\end{equation*}
y s\'olo difiere de la igualdad por una constante que permita que, al integrar sobre todo el soporte de $\theta$, el resultado sea igual a 1.

Cabe resaltar que el factor $\mathbb{P}(y_1,...y_n|\theta)$ es lo que se conoce tambi\'en en otros paradigmas como \textit{verosimilitud}, y que en caso de independencia condicional puede ser reescrito como
\begin{equation*}
    \mathbb{P}(y_1,...,y_n|\theta)  = \prod_{i=1}^n \mathbb{P}(y_i|\theta).
\end{equation*}
Por lo tanto, es posible afirmar que el aprendizaje en el paradigma bayesiano se obtiene como
\begin{equation*}
    Posterior \propto Verosimilitud \times Inicial,
\end{equation*}
es decir, surge de conjuntar el conocimiento inicial con la informaci\'on contenida en los datos.

Es importante notar que bajo este enfoque se obtiene una distribuci\'on de probabilidad completa para  el pron\'ostico de $y_{n+1}$. \'Esta se puede utilizar para el c\'alculo de estimaciones puntuales o intervalos (que en el caso del paradigma bayesiano son llamados de \textit{probabilidad}) mediante funciones de utilidad o p\'erdida, y haciendo uso de la Teor\'ia de la Decisi\'on.

\section{Propiedad conjugada}

En los casos en los que la probabilidad posterior, que resulta del producto de la verosimilitud y la inicial, pertenece a la misma familia de la distribuci\'on inicial, \'unicamente difiriendo en el valor de los par\'ametros, se dice que la distribuci\'on de los par\'ametros y la verosimilitud pertenecen a una \textbf{familia conjugada}.

Esta propiedad es conveniente, porque permite a la distribuci\'on posterior tener forma anal\'itica cerrada, evitando tener que usar m\'etodos num\'ericos para aproximarla. Adem\'as permite ver de forma m\'as clara c\'omo afectan los datos a la actualizaci\'on, respecto a la distribuci\'on inicial.

Algunas de las familias conjugadas m\'as conocidas son la \textit{Normal-Normal}, \textit{Normal-Gamma}, \textit{Normal-Gamma Inversa}, \textit{Bernoulli-Beta} o la \textit{Poisson-Gamma}, donde la primer distribuci\'on representa a la verosimilitud y la segunda, la distribuci\'on de los par\'ametros. 

Sin embargo, el rango de posibles modelos conjugados puede resultar limitado en algunos contextos pr\'acticos debido a que el fen\'omeno en estudio puede ser mejor representado con ciertas distribuciones espec\'ificas, que usualmente no pertenecen a familias conjugadas.

\section[Inferencia con variables predictivas]{Inferencia con variables predictivas\raisebox{.3\baselineskip}{\normalsize\footnotemark}}\footnotetext{Esta secci\'on carece de formalidad, pero busca darle la intuici\'on al lector para generalizar el resultado del teorema de de Finetti en el contexto en el que se desarrollar\'a este trabajo. Para una explicaci\'on formal, consultar \cite{Dawid_Exchangeability}.}

Como se ver\'a en el siguiente cap\'itulo de esta tesis, un problema com\'un es estimar la distribuci\'on de cierta secuencia de variables aleatorias $(y_1,...,y_n)$, condicionadas a los valores $(x_1,...,x_n)$ de otras variables com\'unmente llamadas predictivas. En este caso, la secuencia $(y_1,...,y_n)$ ya no es intercambiable, porque cada valor $y_i$ depende en alguna medida del valor de su respectiva $x_i$, por lo que no es posible aplicar de manera directa el teorema de de Finetti. Para hacerlo de manera indirecta, se introducir\'a el t\'ermino de intercambiabilidad parcial.

\begin{defin*}
    Sea $(y_1|x_1,...,y_n|x_n)$, una secuencia de $n$ variables aleatorias, condicionales en los valores de ciertas variables predictivas, cuya distribuci\'on de probabilidad conjunta est\'a dada por $\mathbb{
    P}(y_1,...,y_n|x_1,...,x_n)$. Sea $\psi$ una funci\'on biyectiva, que va de $\{k_1,...,k_r\} \rightarrow \{k_1,...,k_r\}$, es decir, una funci\'on que crea una permutaci\'on del conjunto $\{k_1,...,k_r\}$; y $\tilde{x}_1,..., \tilde{x}_1$ los distintos valores \'unicos que toman las $x_i's$.  
    Entonces, se dice que $(y_1,...,y_n)$ es una \textbf{secuencia aleatoria infinita y parcialmente intercambiable} si se cumple que 
    \begin{equation*}
        \mathbb{P}(y_{k_1},...,y_{k_r}|\tilde{x}_k) = \mathbb{P}(y_{\psi(k_1)},...,y_{\psi(k_r)}|\tilde{x}_k),
    \end{equation*}
    para cualquier permutaci\'on $\psi$ y para todos los diferentes valores $k$.
    
    Es decir, todas aquellas $y_i's$ cuyas $x_i's$ tienen el mismo valor, son infinitamente intercambiables entre s\'i. 
\end{defin*}

Si adem\'as se cumpliera que el orden de los valores \'unicos $\tilde{x}_k's$ es intercambiable, entonces, intuitivamente se podr\'ia tomar la $G$ del teorema de de Finetti como dependiente de las $x_i$'s, $G(x_i)$, y se obtendr\'ia que
\begin{equation*}
    \mathbb{P}(y_1, ...,y_n|x_1,...,x_n) =
    \int_{\mathcal{F}}\left[\prod_{k=1}^n \mathbb{P}(y_k|G(x_i))\right]dF(G(x_1,...,x_n)),
\end{equation*}
donde las $y_i$'s resultar\'ian ser independientes entre s\'i, condicionadas a una distribuci\'on que depende la $x_i$ asociada.

El tema del siguiente cap\'itulo ser\'a la discusi\'on de m\'etodos de inferencia sobre las variables $y$, dentro de este contexto espec\'ifico de dependencia de una variable predictiva $x$, normalmente conocidos como \textbf{modelos de regresi\'on}.

\newpage