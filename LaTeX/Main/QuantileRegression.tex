\chapter{Regresi\'on sobre cuantiles}

\section{Motivaci\'on}

Cuando surgi\'o entre la comunidad estad\'istica el problema de \textit{regresi\'on sobre cuantiles}, inicialmente fue modelado bajo un enfoque no bayesiano, como se describe en \cite{Yu_BayQuantReg}. 

Sea $q_p$ el cuantil \textit{p}-\'esimo de $X$, es decir, $P(X \leq q_p) = p$. Entonces el cuantil \textit{p}-\'esimo condicional de $y$, dado $x$, se supondr\'a es posible escribirlo como
\begin{equation*}
    q_p(y|x) = x^T\beta(p), 
\end{equation*}
donde $\beta(p)$ es el vector de coeficientes, dependientes de $p$.

Se define una funci\'on de p\'erdida como 
\begin{equation*}
    \rho_p(u) = u \times (p - I_{(u<0)}),
\end{equation*}
misma que se puede reescribir como 
\begin{equation*}
    \rho_p(u) = u \times [pI_{(u>0)} - (1-p) I_{(u<0)})],
\end{equation*}
o
\begin{equation*}
    \rho_p(u) = 
    \frac{|u| + (2p-1)u}{2}.
\end{equation*}

Siguiendo este orden de ideas, se puede demostrar que para el problema de minimizaci\'on
\begin{equation*}
    \begin{aligned}
    \underset{q_p}{\text{min}} \text{ }
    \mathbb{E} [\rho_p(y_i - q_p)],
    \end{aligned}
\end{equation*}
la soluci\'on $q_p^*$ cumple que $P(X \leq q_p^*) = p$.

As\'i, la primera idea para resolver el problema de \textit{regresi\'on sobre cuantiles} fue resolver el problema de minimizaci\'on
\begin{equation*}
\begin{aligned}
\underset{\beta(p)}{\text{min}}
\sum_i \rho_p(y_i - x_i^T \beta(p)).
\end{aligned}
\end{equation*}

Posteriormente, Koenker \& Bassett (1978) retomaron esta idea, aplic\'andola en el paradigma bayesiano. 

\begin{defin}
    Se dice que una variable aleatoria U sigue una distribuci\'on asim\'etrica de Laplace si su funci\'on de densidad se escribe como
    \begin{equation*}
        f_p(u|\mu,\sigma) = 
        \frac{p(1-p)}{\sigma}
        exp\left[
        -\rho_p
        \left(
        \frac{u-\mu}{\sigma}
        \right)
        \right],
    \end{equation*}
con $\mu$ par\'ametro de localizaci\'on y $\sigma$ par\'ametro de escala.
\end{defin}

Es f\'acilmente demostrable que el anterior problema de minimizaci\'on de los errores es equivalente a maximizar la funci\'on de verosimilitud formada como producto de densidades independientes asim\'etricas de Laplace.

Si bien esto represent\'o un gran avance, a\'un queda la posibilidad de retomar estas ideas y crear modelos m\'as precisos. La intenci\'on de esta tesis es encontrar un modelo para la \textit{regresi\'on sobre cuantiles} que sea completamente bayesiano y no param\'etrico, con la intenci\'on de poder representar distribuciones m\'as complejas.

\section[Modelos]{
    Modelos
    \footnote{Los primeros dos son retomados de \cite{Kottas_SemiparamQuantReg}, y el tercero de \cite{Kottas_NotParamQuantReg}.}
}

Esta secci\'on buscar\'a desarrollar modelos que tomen en cuenta los aprendizajes previos, para realizar an\'alisis de \textit{regresi\'on sobre cuantiles}. A lo largo de ella se supondr\'a lo que se anuncia a continuaci\'on.

\begin{defin}
    El \textbf{modelo de regresi\'on del cuantil \textit{p}-\'esimo}, para una variable de respuesta $y \in \mathbb{R}$, y un conjunto de covariables $x \in \mathbb{R}^n$, ser\'a aquel que se pueda escribir como
    \begin{equation*}
        y = f(x) + \varepsilon,
    \end{equation*}
    donde $h: \mathbb{R}^{n} \rightarrow \mathbb{R}$, y $\varepsilon$ es el error aleatorio, independiente de $x$, y cuyo cuantil \textit{p}-\'esimo es igual a $0$. Es decir, si $h_p$ es la funci\'on de densidad de $\varepsilon$, se tiene que
    \begin{equation*}
        \int_{-\infty}^0 h_p(\varepsilon) d\epsilon = p.
    \end{equation*}
\end{defin}

De la definici\'on anterior es posible darse cuenta que el problema de regresi\'on por cuantiles se puede reinterpretar como el problema de encontrar la forma de $f(x)$ y la de $h_p(\varepsilon)$.

Es inmediato darse cuenta que $h_p(\varepsilon)$ tendr\'a una forma sim\'etrica si, y s\'olo si, $p = 0.5$. Es decir, sera sim\'etrica \'unicamente para el modelo de \textit{regresi\'on sobre la mediana}, y asim\'etrica en cualquier otro caso. Intuitivamente, esto provocar\'a que el modelo espere una proporci\'on de errores negativos similar a $p$, y de errores positivos similar a $1-p$, que coincide con lo buscado al realizar un modelo de este tipo.

A continuaci\'on se plantean diversos modelos que realizan supuestos adicionales acerca de la forma de $f(x)$ y de $h_p(\varepsilon)$, para resolver el problema.

\subsection{Mezcla asim\'etrica de densidades de Laplace}

En estos primeros modelos se supondr\'a una forma lineal para $f$, es decir, $f$ se podr\'a escribir como $f(x) = x^T \beta$. Por otro lado, se supondr\'a como forma param\'etrica de $h_p$ a la familia de distribuciones asim\'etricas de Laplace, sin par\'ametro de localizaci\'on, y cuya densidad se escribe como
\begin{equation*}
    w_p^{AL}(\varepsilon,\sigma) = 
    \frac{p(1-p)}{\sigma}
    exp \left(
    - \frac{|\varepsilon| + (2p - 1) \varepsilon}{2 \sigma}
    \right),
\end{equation*}
con par\'ametro de escala $\sigma \in \mathbb{R}^+$ y $0 < p <1$. Dado que es poco com\'un que se conozca el valor de $\sigma$, se recurrir\'a a un modelo de mezclas infinitas de Dirichlet (visto en el cap\'itulo anterior), con par\'ametro de concentraci\'on $\alpha$, y distribuci\'on media $H$, con soporte en $\mathbb{R}^+$. Se define entonces la funci\'on de densidad del error aleatorio como
\begin{equation*}
\begin{aligned}
    h_p^{AL}(\varepsilon|G) &= \int w_p^{AL}(\varepsilon|\sigma)dG(\sigma), \\
    G &\sim DP(\alpha,H).
\end{aligned}
\end{equation*}

Cabe resaltar que a pesar de la mezcla, se sigue cumpliendo la condici\'on de que $\int_{-\infty}^0 h_p^{AL}(\varepsilon|G) d\varepsilon = p$, para cualquier $G$. En cuanto a los par\'ametros del Proceso de Dirichlet, se tomar\'a una $\alpha > 0$, y $H$ ser\'a una Gamma-Inversa, con par\'ametro de forma $c > 0$, y de raz\'on $d > 0$. Siguiendo este orden de ideas, se puede reescribir el modelo como
\begin{equation*}
\begin{aligned}
    y-x^T \beta | \beta, \sigma &\sim w_p^{AL}(\varepsilon | \sigma), \\
    \beta &\sim \mathcal{N}(\beta | m,v), \\
    \sigma | G &\sim G(\sigma), \\
    G | \alpha, c, d &\sim DP(\alpha, H(c,d))
\end{aligned}
\end{equation*}

\subsection{Mezcla de densidades uniformes, con $f(x)$ lineal}

Como se puede corroborar en \cite{Pavlides_NonparamMixUnifDens}, para cualquier funci\'on de densidad $h(\cdot)$ no creciente y con soporte en $R^{+}$, existe una distribuci\'on $G$, tal que
\begin{equation*}
    h(x|G) = \int \theta^{-1} I_{[0,\theta)} (x) dG(\theta).
\end{equation*}

En otras palabras, $h(\cdot)$ puede ser escrita como una mezcla de densidades uniformes.

Siguiendo este orden de ideas, y suponiendo que $G \sim (\alpha, H)$, dicho resultado puede ser usado para definir la parte positiva de la funci\'on de densidad $h_p(\varepsilon)$. Por otro lado, la parte negativa se puede definir de la misma manera usando un signo negativo. Entonces, se tiene que
\begin{equation*}
    h_p(\varepsilon) = \int \int w_p^{DU}(\varepsilon|\sigma_1,\sigma_2)dG_1(\sigma_1)dG_2(\sigma_2),
\end{equation*}
donde $G_1$ y $G_2$ representan a las partes negativas y positivas, respectivamente.

Adem\'as, vale la pena recordar que el cuantil \textit{p}-\'esimo tiene que ser igual a $0$, por lo que el peso de cada uno de los dos lados estar\'a dado por
\begin{equation*}
    w_p^{DU}(\varepsilon|\sigma_1,\sigma_2) =
    \frac{p}{\sigma_1} I_{(-\sigma_1,0)}(\varepsilon) + 
    \frac{(1-p)}{\sigma_2} I_{(0,\sigma_2)}(\varepsilon).
\end{equation*}

De esta manera se tiene un modelo m\'as general que el de Mezcla asim\'etrica de densidades de Laplace, ya que el \'unico supuesto es que la moda est\'a en 0 y la densidad es no creciente hacia las colas. En resumen, el modelo obtenido es
\begin{equation*}
\begin{aligned}
    y-x^T \beta | \beta, \sigma &\sim w_p^{DU}(\varepsilon | \sigma_1, \sigma_2), \\
    \beta &\sim \mathcal{N}(\beta | m,v), \\
    \sigma_r | G_r &\sim G_r(\sigma_r),\\
    G_r | \alpha_r, c_r, d_r &\sim DP(\alpha_r, H(c_r,d_r)), \\
    r &\in \{1,2\}.
\end{aligned}
\end{equation*}

\subsection{Mezcla de densidades uniformes, con Procesos Gaussianos}

Hasta ahora, se ha supuesto que $f(x)$ es lineal. Para darle mayor flexibilidad al modelo, ahora se supondr\'a que $f(x) \sim \mathcal{GP}(m(x),k(x,x))$, con $m$ funci\'on de medias dada por el modelador, y $k$ funci\'on de covarianzas \textit{exponencial cuadrada}, es decir,
\begin{equation*}
    k(x,x'|\lambda^2) = 
    exp\left(-\frac{1}{2}
    \frac{\norm{x-x'}_2^2}{\lambda^2}
    \right),
\end{equation*}
donde, a su vez, $\lambda \sim GI(\lambda_\alpha,\lambda_\beta)$.

AÃ±adiendo esta idea al modelo anterior, se obtiene
\begin{equation*}
\begin{aligned}
    y-f(x) | f(x), \sigma &\sim w_p^{DU}(\varepsilon | \sigma_1, \sigma_2), \\
    f(x) &\sim \mathcal{GP}(m(x),k(x,x|\lambda)), \\
    \lambda &\sim GI(\lambda_\alpha,\lambda_\beta), \\
    \sigma_r | G_r &\sim G_r(\sigma_r),\\
    G_r | \alpha_r, c_r, d_r &\sim DP(\alpha_r, H(c_r,d_r)), \\
    r &\in \{1,2\}.
\end{aligned}
\end{equation*}

\newpage