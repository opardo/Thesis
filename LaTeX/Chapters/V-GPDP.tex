\chapter[Modelo GPDP para regresi\'on sobre cuantiles]{Modelo GPDP para regresi\'on sobre cuantiles}

\section{Definici\'on}

Despu\'es de analizar la introducci\'on de componentes no param\'etricos en las distribuciones, tanto de $f_p$, como de $\varepsilon_p$, a continuaci\'on se enunciar\'a el modelo central de esta tesis, con sus especificaciones correspondientes.

A partir de este punto, a dicho modelo se le denominar\'a \textbf{Modelo GPDP} (por las siglas en ingl\'es de procesos Gaussianos y procesos de Dirichlet).

Sea $\{(y_i,x_i)|i=1,...,m\}$ el conjunto de observaciones de la variable de respuesta y sus respectivas covariables, cuya relaci\'on se supone como
\begin{equation*}
    y = f_p(x) + {\varepsilon_p},
\end{equation*}
donde $f_p: \mathbb{R}^n \times \mathbb{R}$ es la funci\'on tendencia y ${\varepsilon_p} \in \mathbb{R}$ es la dispersi\'on, ambos desconocidos.

Para reflejar la incertidumbre y el conocimiento previo del modelador, se supone a $f_p(X) \sim \mathcal{GP}(m,k)$, con funci\'on de medias $m$ dada por el modelador y funci\'on de covarianza 2-\textit{exponencial}, con par\'ametro de rango fijo $\tau = 1$. Es decir,
\begin{equation*}
    k(x_i, x_j | \lambda, \tau=1) = \lambda  \times exp\{-\norm{x_i - x_j}_2\},
\end{equation*}
con $\lambda \sim GI(c_\lambda,d_\lambda)$, siendo $c_\lambda$ y $d_\lambda$ los par\'ametros de forma y escala, respectivamente, de una \textit{Gamma-Inversa}. 

La raz\'on de fijar $\tau = 1$ es para simplificar el proceso de inferencia que se ver\'a en la siguiente secci\'on, pero bien podr\'ia tambi\'en ser una variable aleatoria.

En cuanto a la distribuci\'on inicial de $\varepsilon_p$, se supondr\'a un modelo de mezclas infinitas de Dirichlet, cuya distribuci\'on media $H$ del proceso de Dirichlet ser\'a una \textit{Gamma-Inversa}, con par\'ametros de forma $c_{DP}$ y escala $d_{DP}$.

En resumen, el Modelo GPDP queda descrito de la siguiente forma:
\begin{equation*}
\begin{aligned}
    y_i| f_p(x_i), z_i, \sigma_k^* &\sim AL_p({\varepsilon_p}_i = y_i - f_p(x_i) | \sigma_{z_i}), \\
    f_p(X)|m, \lambda &\sim \mathcal{GP}(m,k(\lambda)|\lambda), \\
    \lambda &\sim GI(c_\lambda,d_\lambda), \\
    z_i | \pi &\sim Mult(\pi), \\
    \pi | \alpha &\sim GEM(\alpha), \\
    \sigma_k^* | c_{DP}, d_{DP} &\sim GI(\sigma_k|c_{DP}, d_{DP}),\\
    k(x_i, x_j | \lambda) &= \lambda  \times exp\{-\norm{x_i - x_j}_2\}.
\end{aligned}
\end{equation*}

\section{Inferencia con el simulador de Gibbs}

Dado que el modelo descrito no es conjugado, las distribuciones posteriores tienen que ser aproximadas mediante m\'etodos computacionales. Para hacer esto, se puede hacer uso de algoritmos MCMC (Monte Carlo Markov Chains), y particularmente del simulador de Gibbs. \footnote{En caso de que el lector no est\'e familiarizado con este tipo de algoritmos, puede consultar una breve descripci\'on de ellos en el \autoref{chap:MCMC}.}

En este orden de ideas, a continuaci\'on se detallan las distribuciones condicionales posteriores de los par\'ametros del modelo, as\'i como la inclusi\'on de algunas variables latentes para permitir el funcionamiento del algoritmo.

Cabe aclarar que antes de correr los algoritmos, resulta conveniente primero estandarizar los datos. En primer lugar, para que la estructura de covarianza tenga más sentido, ya que la escala de las covariables afectaría la correlación que existe entre los datos, al depender esta de la distancia absoluta entre ellas. Además, estandarizar los datos suele mejorar el rendimiento computacional de este tipo de algoritmos. Asimismo, vuelve m\'as sencillo definir el valor inicial de los par\'ametros, como se detallar\'a m\'as adelante.

\subsection{Actualizaci\'on de la dispersi\'on}

Recordando que los centros de masa y los pesos del Proceso de Dirichlet son independientes, pueden ser actualizados por separado, con el inconveniente de que hay un n\'umero infinito de par\'ametros que actualizar. Para resolverlo, se utilizará el algoritmo de truncamiento del \textit{slice sampling}, propuesto por \cite{Kalli_Slice}, y adaptado para el modelo propuesto en esta tesis.

Sea $\xi_1,\xi_2,\xi_3,...$ una secuencia positiva, generalmente elegida determinista y decreciente. Sea $N$ una variable aleatoria con soporte en los enteros positivos, una variable auxiliar incorporada al modelo.

\subsubsection{Actualizaci\'on de los centros de masa}

Para cada $k \in \{1,2,...,N\}$,
\begin{equation*}
\begin{gathered}
    \sigma_k | \{{\varepsilon_p}_i, z_i | z_i = k\}, c, d \sim GI(\bar{c}_{DP}, \bar{d}_{DP}),\\
    \bar{c}_{DP} = c_{DP} + |\{i| z_i = k\}|, \\
    \bar{d}_{DP} = d_{DP} 
    + p \left[\sum_{\{i| z_i = k,\text{ }{\varepsilon_p}_i \geq 0\}} {\varepsilon_p}_i\right]
    + (1-p) \left[\sum_{\{i| \text{ } z_i = k,\text{ }{\varepsilon_p}_i < 0\}}  -{\varepsilon_p}_i\right].
\end{gathered}
\end{equation*}

\subsubsection{Actualizaci\'on de los pesos}

Sea $\hat{\pi}_k = \beta_k \prod_{j=1}^{k-1}(1 - \beta_j)$, de modo que para cada $k \in \{1,2,...,N\}$,
\begin{equation*}
\begin{aligned}
    \beta_k|\{z_i\}, a,b &\sim Beta(\bar{a}, \bar{b}), \\
    \bar{a} &= 1 + |\{i|z_i = k\}|, \\
    \bar{b} &= \alpha + |\{i|z_i > k\}|.
\end{aligned}
\end{equation*}

Entonces, se calcula
\begin{equation*}
\begin{aligned}
    \pi_k = \frac{\bar{\pi_k}}{\sum_{j=1}^N \bar{\pi_j}}
\end{aligned}
\end{equation*}

\subsubsection{Actualizaci\'on de las clases y variables de truncamiento}

Para cada observaci\'on $i \in \{1,...,m\}$, se obtiene
\begin{equation*}
\begin{aligned}
   u_i \sim U(0, \xi_{z_i}),
\end{aligned}
\end{equation*}
valor que se utiliza para actualizar la probabilidad de pertenencia a cada clase de la siguiente forma. Para cada $k \in \{1,2,...,N\}$,
\begin{equation*}
\begin{aligned}
   P(z_i = k| {\varepsilon_p}_i, \pi_k, \sigma_k)
   \propto
   \mathds{1}(\xi_k > u_i)
   \cdot
   \frac{\pi_k}{\xi_k}
   \cdot
   AL_p({\varepsilon_p}_i | \sigma_k).
\end{aligned}
\end{equation*}

Posteriormente se actualiza
\begin{equation*}
\begin{aligned}
   N = \max\{
    N_i|N_i=\max\{j|\xi_j > u_i\}, 
    i \in \{1,...,m\}
   \}.
\end{aligned}
\end{equation*}

\subsection{Actualizaci\'on de la tendencia}

Se define la variable aleatoria auxiliar:
\begin{equation*}
\begin{aligned}
    b_i &\sim 
    \begin{cases}
        \frac{p}{\sigma_i} &prob = P({\varepsilon_p}_i \geq 0) = 1-p\\
        -\frac{1-p}{\sigma_i} &prob = P({\varepsilon_p}_i < 0) = p
    \end{cases},\\
\end{aligned}
\end{equation*}
de forma que $b = [b_1,...,b_m]^T$. 

\subsubsection{Actualizaci\'on de $\bm{f_p(x)}$}

Es posible calcular que
\begin{equation*}
\begin{aligned}
   f_p(x)|Y,X,M_{f_p}(X),b,\lambda &\sim TruncNormal(\bar{M}_{f_p}(X,b), K_{f_p}(X,X|\lambda), \gamma, \eta), \\
   \bar{M}_{f_p}(X,b) &= M_{f_p}(X) + K_{f_p}(X,X|\lambda)b, \\
   \gamma_i &= 
   \begin{cases}
    -\infty & \text{si }b_i > 0 \\
    y_i & \text{si }b_i < 0
   \end{cases},\\
   \eta_i &= 
   \begin{cases}
    y_i & \text{si }b_i > 0 \\
    \infty & \text{si }b_i < 0
   \end{cases},
\end{aligned}
\end{equation*}
donde $\gamma$ es el vector de l\'imites inferiores y $\eta$ es el vector de l\'imites superiores de la distribuci\'on \textit{Normal truncada}.

\subsubsection{Actualizaci\'on del par\'ametro de escala}

Por otro lado, se puede obtener que
\begin{equation*}
\begin{gathered}
   P(\lambda|X,M_{f_p}(X),f_p(X),b,c_\lambda,d_\lambda) 
   \propto
   \lambda^{-\bar{c}_\lambda-1}
   \cdot
   exp\left\{- \frac{\bar{d}_\lambda}{\lambda}\right\}
   \cdot
   exp\left\{-\bar{B} \lambda\right\}, \\
   \bar{c}_\lambda = c_\lambda + \frac{p}{2}, \\
   \bar{d}_\lambda = d_\lambda + \bar{F}, \\
   \bar{F} = \frac{1}{2}(f_p(X)-M_{f_p}(X))^T [K_{f_p}(X,X|\lambda=1)^{-1}] (f_p(X)-M_{f_p}(X)), \\
   \bar{B} = \frac{1}{2}b^T [K_{f_p}(X,X|\lambda=1)] b.
\end{gathered}
\end{equation*}

\section{Predicci\'on}

Una de las desventajas de los modelos no param\'etricos es que, a diferencia de los modelos param\'etricos, es complicado interpretar los resultados del ajuste del modelo.

Por ello, resulta particularmente importante la faceta de la predicci\'on, que es la que m\'as explota sus ventajas, y en la que los modelos param\'etricos normalmente se quedan cortos. Espec\'ificamente esta secci\'on se enfocar\'a en la predicci\'on de $f_p$, que es el par\'ametro de mayor inter\'es del modelo.

Debido al uso del simulador de Gibbs, despu\'es de realizar el ajuste se cuenta con un conjunto grande de realizaciones aproximadas de $f_p(X)$, provenientes de las cadenas de Markov.

Recordando lo visto en la secci\'on 4.2.4, cuando se tienen valores de $f_p(X)$, es posible usar la propiedad de la \textit{Normal condicional} para realizar predicci\'on. Sea $X \in \mathbb{R}^m \times \mathbb{R}^n$ la matriz de datos originales, $X_* \in \mathbb{R}^r \times \mathbb{R}^n$ la matriz de datos a predecir, $f_p(X)$ una realizaci\'on de la distribuci\'on posterior correspondiente a X, y $f_p(X_*)$ el vector aleatorio de los datos a predecir. Se tiene entonces que 
\begin{equation*}
    f_p(X_*)|f_p(X) 
    \sim \mathcal{N}
    (\bar{M}(X,X_*),\bar{K}(X,X_*|\lambda)),
\end{equation*}
con
\begin{equation*}
\begin{aligned}
    \bar{M}(X,X_*) &= M(X_*) + K(X_*,X)K(X,X)^{-1}(f_p(X) - M(X)), \\
    \bar{K}(X,X_*|\lambda) &= 
    \lambda
    \times
    \left[
    K(X_*,X_*) -
    K(X_*,X)K(X,X)^{-1}K(X,X_*)
    \right]
    .
\end{aligned}
\end{equation*}
donde $K(X_1,X_2) = K(X_1,X_2|\lambda=1)$, y $X_1$ y $X_2$ pueden ser $X$ o $X_*$.

Por lo antes descrito, es posible obtener una realizaci\'on de $f_p(X_*)$ simulando de dicha distribuci\'on \textit{Normal}. De esta manera, por cada valor de $f_p(X)$ y $\lambda$ en la cadena de Markov, se simula una realizaci\'on de $f_p(X_*)$, y entonces es posible aproximar la distribuci\'on posterior de $q_p(y|x)$, para los datos $X_*$.

\section{Hiper-par\'ametros iniciales}

Una complicaci\'on que puede tener un modelo con la complejidad del GPDP es que los hiper-par\'ametros que tiene que definir el modelador no son inmediatos, sino est\'an en la profunidad de un conjunto jer\'arquico de distribuciones. Por ello, no resulta sencillo asignarles valores iniciales.

Para mitigar este problema, a continuaci\'on se proponen una serie de heur\'isticas para su c\'alculo, mismas que se derivan de algunas ideas que me parecen sensatas, pero no se originan de ning\'un cuerpo axiom\'atico y bien podr\'ian ser mejoradas. Tambi\'en es importante aclarar que por lo comentado al inicio de la secci\'on 5.2, para todas ellas se pensar\'a que los datos est\'an estandarizados.

\subsection{Funci\'on de medias $m_{f_p}$}

Para asignar la funci\'on de medias del proceso Gaussiano, se puede partir de la hip\'otesis que $m_{f_p}$ es constante, y, por lo tanto, las variaciones son \'unicamente producto de la varianza de $f_p$ y $\varepsilon_p$. Dada la estructura de probabilidad posterior, la media de $f_p(x)$ podr\'a actualizarse si los datos cuentan con informaci\'on suficiente para suponer lo contrario. 

Una vez aceptada esta estructura para definir a la funci\'on de medias, resta asignar el valor constante que tomar\'a, siendo una idea el asignar el cuantil muestral $Q_p(y)$ de los datos de la variable de respuesta $y$, es decir,
\begin{equation*}
\begin{aligned}
    m_{f_p}&:\mathbb{R}^n \rightarrow \mathbb{R} \mid \\
    m_{f_p}&(x) = Q_p(y).
\end{aligned}
\end{equation*}

\subsection{\textit{Gamma-Inversa}s de $\lambda$ y el Proceso de Dirichlet}

Tanto $c_\lambda$ y $d_\lambda$, como $c_{DP}$ y $d_{DP}$ son par\'ametros de distribuciones \textit{Gamma-Inversa}. Es oportuno recordar que si $U \sim \mathcal{GI}(c,d)$, entonces
\begin{equation*}
\begin{aligned}
    \mathbb{E}[U] &= \frac{d}{c-1}, \text{ } c>1,\\
    Var(U) &= \frac{d^2}{(d-1)^2(d-2)}, \text{ } c>2.
\end{aligned}
\end{equation*}

Por lo tanto, eligiendo $c = 2$, $Var(U)$ ser\'a infinita y $\mathbb{E}[U] = d$. Asignar a $c_\lambda$ y $c_{DP}$ de esta manera permitir\'a darle a $d_\lambda$ y $d_{DP}$ el valor que se piense como el mejor estimador puntual \textit{a priori} de $\lambda$ y $\sigma$, pero con una varianza grande y cola pesada, que permitir\'a a los datos tener el peso principal en el ajuste del modelo. 

Debido a la estandarizaci\'on de los datos, la varianza muestral de $y$ es igual a 1. Es posible pensarla como el resultado de sumar la varianza de $f_p(x)$ y la de $\varepsilon_p$, que adem\'as se suponen independientes. Entonces, se puede definir una heur\'istica tal que $Var(f_p(x)) = \frac{1}{2}$ y $Var(\varepsilon_p) = \frac{1}{2}$, a falta de mayor informaci\'on.

La varianza de $f_p(x)$ es igual a $\lambda$, por lo que lo coherente con lo dicho en los p\'arrafos anteriores ser\'a asignar $d_\lambda = \frac{1}{2}$. 

Por el otro lado, si \'unicamente para este ejercicio, y con el af\'an de volver an\'alitico el c\'alculo, se piensa a $\varepsilon_p \sim AL_p(\sigma = d_{DP})$. Entonces, su varianza estar\'ia dada por
\begin{equation*}
    Var(\varepsilon_p) = 
    \left[\frac{d_{DP}}{p(1-p)}\right]^2
    (1-2p(1-p)).
\end{equation*}
Dado que se fijar\'a $Var(\varepsilon_p) = \frac{1}{2}$, por la heur\'istica antes mencionada, despejando es posible obtener que
\begin{equation*}
    d_{DP} = \frac{p(1-p)}{\sqrt{2(1-2p(1-p))}}.
\end{equation*}

\subsection{Par\'ametro de concentraci\'on $\alpha$}

Este es el par\'ametro m\'as dif\'icil de definir, por su complejidad de interpretaci\'on. Pero cabe recordar que el valor de $\alpha$ tiene una relaci\'on positiva con el n\'umero de subpoblaciones. 

De hecho, sea $\bar{m}$ el n\'umero de subpoblaciones y $m$ el n\'umero de datos de entrenamiento, \cite{Yee_DirProc} expone que
\begin{equation*}
    \mathbb{E}[\bar{m}|\alpha, m] 
    \simeq 
    \alpha
    \log 
    \left(
        1 + \frac{m}{\alpha}
    \right)
    \text{, para } m, \alpha \gg 0.
\end{equation*}

Si se define $\alpha = \frac{\sqrt{m}}{2}$, se tiene que
\begin{equation*}
\begin{aligned}
    \mathbb{E}[\bar{m}|m] 
    &\simeq 
    \frac{\sqrt{m}}{2}
    \times
    \log 
    \left(
        1 + 2\sqrt{m}
    \right)\\
    &\simeq
    \frac{m}{7} 
    \text{, para } m \approx 100.
\end{aligned}
\end{equation*}

Es decir, si se tienen alrededor de 100 observaciones, el n\'umero esperado de subpoblaciones ser\'a alrededor de la s\'eptima parte de las observaciones. Valor que a falta de mayor exploraci\'on en este tema, parece sensato.

\section{Paquete \textit{GPDPQuantReg} en R}

Todas las ideas expuestas en este cap\'itulo han sido implementadas en el paquete \textit{GPDPQuantReg} del lenguaje de programaci\'on R, mismo que puede ser encontrado en el repositorio de Github\faGithub \space titulado: \textbf{opardo/GPDPQuantReg}.

Al momento de escribir este trabajo, cuenta con tres funciones p\'ublicas: \textit{GPDPQuantReg}, para ajustar el modelo con el simulador de Gibbs; \textit{predict}, para realizar predicci\'on en un nuevo conjunto de datos del modelo ajustado; y \textit{diagnose}, para realizar el diagn\'ostico de la ergodicidad, la autocorrelaci\'on, la correlaci\'on cruzada y la traza de las cadenas de Markov, para los distintos par\'ametros.

A continuaci\'on se expone un ejemplo de uso, el cual es similar a lo que se realiz\'o para obtener los resultados del cap\'itulo siguiente.

\lstinputlisting{R/package_example.R}

\newpage