\chapter[Paradigma Bayesiano]{Paradigma bayesiano\footnote{Las ideas de este cap\'itulo son retomadas de \cite{Denison_BayesMethods}.}\textsuperscript{,}\footnote{Esta tesis da como aceptados los axiomas de coherencia de la Teoría de la Decisión, mismos que pueden ser encontrados, por ejemplo, en \cite{Fishburn_Axioms}. Por lo tanto, entiende al paradigma Bayesiano como el coherente para hacer estad\'istica, cuando una toma de decisi\'on con incertidumbre es el objetivo final del estudio. 
}}
\label{chap:Bayesian}

\section{Inferencia de variables aleatorias}

Un problema clásico de la estad\'istica es el de hacer predicci\'on, utilizando la informaci\'on de los datos que ya han sido observados. Por ejemplo, es posible pensar que ya se tiene el conjunto de $n$ datos observados $\{y_1, \ldots, y_n\}$ y se desea hacer predicci\'on acerca del valor del dato $y_{n+1}$, que a\'un no ha sido observado. Para esto, se podr\'ia usar la probabilidad condicional
\begin{equation*}
    \mathbb{P}(y_{n+1}|y_1,\ldots,y_n) =
    \frac{\mathbb{P}(y_{n+1} \cap \{y_1, \ldots, y_n\})}{\mathbb{P}(y_1, \ldots, y_n)} =
    \frac{\mathbb{P}(y_1, \ldots, y_n,y_{n+1})}{\mathbb{P}(y_1, \ldots, y_n)},
\end{equation*}
pero esto requerir\'ia conocer la funci\'on conjunta, misma que puede ser compleja por la estructura de dependencia de los datos.\footnote{En este trabajo se usar\'a la notaci\'on $\mathbb{P}$ como una forma general de definir una medida de probabilidad, independientemente de los detalles te\'oricos sobre an\'alisis y medibilidad.}

Este problema puede ser abordado mediante el uso del Teorema de representaci\'on general de de Finetti. Para ello, antes se dar\'a una definici\'on.

\begin{defin*}
    Sea $(y_1,y_2,\ldots)$, una sucesi\'on de variables aleatorias, cuya distribuci\'on de probabilidad conjunta est\'a dada por $\mathbb{P}(y_1,y_2,\ldots)$. Sea $\psi$ una funci\'on biyectiva que crea una permutaci\'on finita del conjunto $\{1,2,\ldots\}$, es decir, permuta un n\'umero finito de elementos y al resto los deja fijos.  
    Se dice entonces que $(y_1,y_2,\ldots)$ es una \textbf{sucesi\'on aleatoria infinitamente intercambiable} si se cumple que 
    \begin{equation*}
        \mathbb{P}(y_1,y_2,\ldots) = \mathbb{P}(y_{\psi(1)},y_{\psi(2)},\ldots),
    \end{equation*}
    para cualquier permutaci\'on $\psi$.
\end{defin*}

En pocas palabras, una sucesi\'on $(y_1,y_2,\ldots)$ se considerar\'a infinitamente intercambiable si el orden en que se etiquetan las variables no afecta su distribuci\'on conjunta. Es importante hacer notar que la com\'unmente usada independencia implica intercambiabilidad, pero lo contrario no se cumple. Es decir, la intercambiabilidad es un supuesto menos r\'igido que la independencia.

Dicho esto, es momento de plantear el \textbf{\textit{Teorema de represesentaci\'on general de de Finetti}}.\footnote{Una demostraci\'on de este teorema puede ser encontrada en \cite{Schervish_TheoryStats}.}

\begin{theorem*}
    Sea $(y_1,y_2,\ldots)$ una sucesi\'on aleatoria infinitamente intercambiable de valores reales. Entonces existe una distribuci\'on de probabilidad $F$ sobre $\mathcal{F}$, el espacio de todas las distribuciones, de forma que la probabilidad conjunta de $(y_1,y_2,\ldots)$ se puede expresar como
    \begin{equation*}
        \mathbb{P}(y_1,y_2,\ldots) =
        \int_{\mathcal{F}}\left[\prod_{k=1}^\infty \mathbb{P}(y_k|G)\right]dF(G),
    \end{equation*}
    con
    \begin{equation*}
        F(G) = \lim_{n \to \infty} F(G_n),
    \end{equation*}
    donde $F(G_n)$ es una funci\'on de distribuci\'on evaluada en la funci\'on de distribuci\'on emp\'irica definida por
    \begin{equation*}
        G_n(y) = \frac{1}{n} \sum_{i=1}^n I(y_i \leq y).
    \end{equation*}
    En otras palabras, el Teorema de de Finetti dice que $\{y_1,y_2,\ldots\}$ es un conjunto de variables aleatorias condicionalmente independientes, dado que se supone provienen de la distribuci\'on $G$. Al mismo tiempo, $G$ es una realizaci\'on de otra variable aleatoria cuya distribuci\'on est\'a dada por $F(G)$.
\end{theorem*}

Cabe hacer notar que dicho teorema plantea la distribuci\'on conjunta de $(y_1,y_2,\ldots)$ como una mezcla de verosimilitudes condicionalmente independientes en $G$, donde el peso asociada a cada una depende de $F(G)$. Por lo tanto, $F(G)$ expresa la incertidumbre acerca de cu\'an probable es que $G$ sea id\'onea para explicar el fen\'omeno, a\'un sin observar los datos.

Un subconjunto del espacio de todas las distribuciones $\mathcal{F}$ es el espacio de las distribuciones param\'etricas, es decir, aquellas que pueden ser descritas en su totalidad \'unicamente señalando el valor de un vector de par\'ametros $\theta$ de tamaño finito, mismo que puede tomar valores en todo un soporte $\Theta$\footnote{En lo que resta de esta secci\'on las proposiciones y resultados har\'an referencia al conjunto de distribuciones param\'etricas, apelando a que as\'i los nuevos conceptos tengan mayor claridad para el lector, considerando que este tipo de distribuciones son con las que se suele estar m\'as familiarizado. Todos ser\'an generalizables al conjunto de distribuciones $\mathcal{F}$.}. Por lo tanto, si se hace el supuesto adicional que la distribuci\'on marginal de $y_i$ es param\'etrica, con vector de par\'ametros desconocido $\theta$, se obtiene como corolario del Teorema de de Finetti que
\begin{equation*}
    \mathbb{P}(y_1,y_2,\ldots) =
    \int_{\Theta}\left[\prod_{k=1}^\infty \mathbb{P}(y_k|\theta)\right]\mathbb{P}(\theta)d\theta.
\end{equation*}

Siguiendo el razonamiento anterior, $\mathbb{P}(\theta)$ refleja la incertidumbre del modelador acerca de cu\'al es el vector $\theta$ que origin\'o los datos, antes de observarlos. 

La intuici\'on detr\'as de este resultado es que, al igual que en otros paradigmas, se supone a un s\'olo vector $\theta$ como aquel del que provinieron los datos, pero es desconocido, y la tarea es estimarlo. Una particularidad del paradigma Bayesiano es expresar la incertidumbre que tiene el modelador acerca del valor verdadero mediante la asignaci\'on de una distribuci\'on de probabilidad para $\theta$, sujeta a la informaci\'on inicial o conocimiento previo a observar los datos $(CP)$ que se tenga del fen\'omeno. Es decir, $\mathbb{P}(\theta|CP)$. Como una simplificaci\'on de la notaci\'on, en la literatura normalmente se escribe $\mathbb{P}(\theta) = \mathbb{P}(\theta|CP)$ y se conoce como la \textit{probabilidad inicial} del par\'ametro.

As\'i, el Teorema de representaci\'on general garantiza que para variables aleatorias infinitamente intercambiables existe una distribuci\'on del vector de par\'ametros, tal que la probabilidad conjunta se puede expresar como la verosimilitud de variables condicionalmente independientes, dado un vector de par\'ametros, multiplicada por la probabilidad de que dicho vector de par\'ametros sea del que efectivamente provinieron las observaciones. Con el teorema se prueba la existencia de dicha representaci\'on, mas no su unicidad. En los siguientes p\'arrafos se har\'a uso de ella para realizar inferencia en variables aleatorias.

Regresando al problema inicial, y bajo los supuestos reci\'en mencionados, es posible escribir
\begin{equation*}
    \mathbb{P}(y_{n+1} | y_1,\ldots,y_n) =
    \int_{\Theta} 
    \mathbb{P}(y_{n+1}|\theta) \mathbb{P}(\theta|y_1,\ldots,y_n)d\theta,
\end{equation*}
donde a su vez, usando el \textbf{Teorema de Bayes}, se obtiene que
\begin{equation*}
\begin{aligned}
    \mathbb{P}(\theta|y_1,\ldots,y_n) &=
    \frac{\mathbb{P}(y_1,\ldots,y_n|\theta)\times\mathbb{P}(\theta)}
    {\mathbb{P}(y_1,\ldots,y_n)},
\end{aligned}
\end{equation*}
que en el paradigma Bayesiano se conoce como la \textit{probabilidad posterior} del par\'ametro. 

Se puede observar que el denominador es constante respecto a $\theta$, por lo que usualmente la probabilidad condicional del vector de par\'ametros no se expresa como una igualdad, sino con la expresi\'on
\begin{equation*}
    \mathbb{P}(\theta|y_1,\ldots,y_n) 
    \propto 
    \mathbb{P}(y_1,\ldots,y_n|\theta) \times \mathbb{P}(\theta),
\end{equation*}
y s\'olo difiere de la igualdad por una constante que permita que, al integrar sobre todo el soporte de $\theta$, el resultado sea igual a 1.

Cabe resaltar que debido a la independencia condicional dada por el Teorema de de Finetti, el factor de verosimilitud puede ser reescrito como
\begin{equation*}
    \mathbb{P}(y_1,\ldots,y_n|\theta)  = \prod_{i=1}^n \mathbb{P}(y_i|\theta).
\end{equation*}
Por lo tanto, se confirma que el aprendizaje en el paradigma Bayesiano se obtiene como
\begin{equation*}
    Posterior \propto Verosimilitud \times Inicial.
\end{equation*}
Es decir, el conocimiento final surge de conjuntar el conocimiento inicial con la informaci\'on contenida en los datos.

Es importante notar que bajo este enfoque se obtiene una distribuci\'on de probabilidad para un valor futuro de $y_{n+1}$. \'Esta se puede utilizar para el c\'alculo de predicciones puntuales o intervalos (que en el caso del paradigma Bayesiano son de \textit{probabilidad}), mediante el uso de la Teor\'ia de la Decisi\'on y funciones de utilidad o p\'erdida.

\section{Propiedad conjugada}

En los casos en los que la distribuci\'on posterior de los par\'ametros tiene la misma forma funcional que la distribuci\'on inicial, se dice que la distribuci\'on de los par\'ametros pertenece a una \textbf{familia param\'etrica conjugada}, para la verosimilitud respectiva.

Esta propiedad es conveniente, porque permite a la distribuci\'on posterior tener forma anal\'itica cerrada, evitando tener que usar m\'etodos num\'ericos para aproximarla. Adem\'as permite ver de forma m\'as clara c\'omo afectan los datos a la actualizaci\'on, respecto a la distribuci\'on inicial.

Sin embargo, el rango de posibles modelos conjugados puede resultar limitado en algunos contextos pr\'acticos debido a que el fen\'omeno en estudio puede ser mejor representado con otras distribuciones, que usualmente no pertenecen a familias conjugadas.

\section[Inferencia con variables explicativas]{Inferencia con variables explicativas\raisebox{.3\baselineskip}{\normalsize\footnotemark}}\footnotetext{Esta secci\'on carece de una formalidad completa, pero busca darle la intuici\'on al lector para generalizar el resultado del Teorema de de Finetti en el contexto en el que se desarrollar\'a este trabajo. Para una explicaci\'on m\'as formal, consultar \cite{Dawid_Exchangeability}.}

Como se ver\'a en el siguiente cap\'itulo de esta tesis, un problema com\'un es estimar la distribuci\'on de cierta sucesi\'on de variables aleatorias $(y_1,y_2,\ldots)$, condicionada a las observaciones $(x_1,x_2,\ldots)$ de otras variables usualmente llamadas explicativas o predictivas (cada $x_i$ es un vector de dimensi\'on finita). En este caso, la sucesi\'on $(y_1,y_2,\ldots)$ ya no es intercambiable, porque cada valor $y_i$ depende del valor de su respectiva $x_i$, por lo que no es posible aplicar de manera directa el Teorema de de Finetti. Para hacerlo de manera indirecta, se introducir\'a el concepto de intercambiabilidad parcial.

\begin{defin*}
    Sea $(y_1,y_2,...)$ dada $(x_1,x_2,...)$, una sucesi\'on numerable de variables aleatorias, asociada con los correspondientes valores de sus variables explicativas, y cuya distribuci\'on de probabilidad conjunta est\'a dada por $\mathbb{P}(y_1,y_2,\ldots|x_1,x_2,\ldots)$. Sea $\psi$ una funci\'on biyectiva que crea una permutaci\'on finita de un conjunto, es decir, permuta un n\'umero finito de elementos y al resto los deja fijos. Sean $\tilde{x}_1,\tilde{x}_2,\ldots$ los distintos valores \'unicos que toman las $x$'s, y $y_{k_i}$ una $y$, tal que el valor de su correspondiente variable explicativa es $\tilde{x}_k$.  
    Entonces, se dice que $(y_1,y_2,\ldots)$ es una \textbf{sucesi\'on aleatoria infinita y parcialmente intercambiable} si se cumple que 
    \begin{equation*}
        \mathbb{P}(y_{k_1},y_{k_2},\ldots|\tilde{x}_k) = \mathbb{P}(y_{\psi(k_1)},y_{\psi(k_2)},\ldots|\tilde{x}_k),
    \end{equation*}
    para cualquier permutaci\'on $\psi$ y para todos los diferentes valores $k$.
    
    Es decir, todas aquellas $y$'s cuyas $x$'s tienen el mismo valor, son infinitamente intercambiables entre s\'i. 
\end{defin*}

Si adem\'as se cumpliera que el orden de los valores \'unicos $\tilde{x}$'s es intercambiable en el sentido de de Finetti, entonces, intuitivamente se podr\'ia tomar la $G$ del Teorema de de Finetti como dependiente de las $x_i$'s, $G(x_i)$, y se obtendr\'ia que
\begin{equation*}
    \mathbb{P}(y_1,y_2,\ldots|x_1,x_2,\ldots) =
    \int_{\mathcal{F}}\left[\prod_{i=1}^\infty \mathbb{P}(y_i|G(x_i))\right]dF(G(x_1,x_2,\ldots)),
\end{equation*}
donde las $y$'s resultar\'ian ser independientes entre s\'i, condicionadas a una distribuci\'on que depende de la $x_i$ asociada.

El tema del siguiente cap\'itulo ser\'a la discusi\'on de m\'etodos de inferencia sobre las variables $y$, dentro de este contexto espec\'ifico de dependencia de una variable explicativa o predictiva $x$, normalmente conocidos como \textbf{modelos de regresi\'on}.

\newpage