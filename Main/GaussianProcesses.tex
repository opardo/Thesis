\chapter[Procesos Gaussianos]{Procesos Gaussianos}

\section{Motivaci\'on}

\subsection[Modelos de regresi\'on no lineal]{
    Modelos de regresi\'on no lineal
}

En el cap\'itulo anterior se analiz\'o un modelo robusto para realizar regresi\'on hacia una variable de respuesta, dado un cierto conjunto de covariables. Si bien es un modelo con muchas ventajas, es relevante no olvidar que cuenta con un supuesto fuerte: la relación entre la variable dependiente $y$ y las variables independientes $x$ \'unicamente se da de forma lineal. Pero las funciones lineales s\'olo son un pequeño subconjunto del conjunto infinito no-numerable de funciones existentes. Por ello, valdr\'ia la pena analizar si es posible relajar este supuesto y tener un modelo m\'as general.

Una idea inicial para darle la vuelta a este supuesto es redefinir variables, de tal manera que se pueda obtener un polinomio. Por ejemplo, pensemos que $\hat{x}$ es un buen predictor de $y$, pero como polinomio de orden 3, es decir:
\begin{equation*}
    y = \beta_0 + \beta_1\hat{x} + \beta_2\hat{x}^2 + \beta_3\hat{x}^3 + \varepsilon.
\end{equation*}

Entonces, se puede definir el vector $x$ de covariables como $x = (1,\hat{x},\hat{x}^2,\hat{x}^3)$ y aplicar las t\'ecnicas de regresi\'on lineal ya mencionadas.

Otra cr\'itica que se le podr\'ia hacer a este modelo es la rigidez en la interacci\'on entre variables. Para ejemplificar esto, se podr\'ia pensar en un modelo de la forma:
\begin{equation*}
    y = \beta_0 + \beta_1\hat{x}_1 + \beta_2\hat{x}_2 + \beta_3\hat{x_1}\hat{x_2} + \varepsilon.
\end{equation*}

Es posible entonces declarar el vector $x$ de variables de entrada de la forma $x = (1,\hat{x}_1,\hat{x}_2,\hat{x}_1\hat{x}_2)$, y el procedimiento ser\'ia an\'alogo.

Y a\'un es posible dar un siguiente paso, saliendo del terreno de los polinomios y entrando en el de las funciones biyectivas. Se podr\'ia pensar en un caso como el siguiente (donde siempre se cumpla que $\hat{y} > 1$):
\begin{equation*}
\begin{aligned}
    ln(\hat{y}) &= \hat{\beta_0}\hat{x}_1^{\beta_1}\hat{x}_2^{\beta_2} e^{\varepsilon} \\
    \implies ln(ln(\hat{y})) &= ln(\hat{\beta_0}) + \beta_1 ln(\hat{x}_1) + \beta_2 ln(\hat{x}_2) + \varepsilon \\
    \implies y &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon, 
\end{aligned}
\end{equation*}
donde
\begin{equation*}
\begin{aligned}
    y &= ln(ln(\hat{y})), \\
    \beta_0 &= ln(\hat{\beta_0}), \\
    x_1 &= ln(\hat{x}_1), \\
    x_2 &= ln(\hat{x}_2),
\end{aligned}
\end{equation*}
y el procedimiento se convierte en el ya conocido.

Si bien estos ejemplos permiten ampliar el conjunto de funciones que es posible cubrir usando un modelo de \textit{regresi\'on lineal sobre la media}, permiten darse cuenta de c\'omo se puede complicar la relaci\'on de dependencia entre $y$ y las covariables $x$, de tal manera que muchas funciones pueden no ser descritas con el m\'etodo antes planteado.

As\'i surge la necesidad de buscar un m\'etodo que permita encontrar cualquier tipo relaci\'on entre $y$ y $x$, sin restringirla a un pequeño subconjunto de funciones. El reto es que \'unicamente se tiene tiempo finito para encontrar la mejor estimaci\'on, entre una infinidad no-numerable de opciones.

\subsection[Introducci\'on a los Procesos Gaussianos]{
    Introducci\'on a los Procesos Gaussianos
    \footnote{Las ideas de esta subsecci\'on y de lo que resta del cap\'itulo son inspiradas por \cite{Rasmussen_GauProc}.}
}

Para relajar el supuesto de linearidad, se puede pensar que la relación entre la variable de salida $y$ y las covariables $x$ se da mediante cierta función general $f: \mathbb{R}^n \rightarrow \mathbb{R}$. De esta forma, el modelo se plantea como:
\begin{equation*}
    y = f(x) + \epsilon \sim \mathcal{N}(f(x),\sigma^2).
\end{equation*}

Para continuar con la notación matricial del modelo anterior, sean $Y \in \mathbb{R}^m$ y $X \in \mathbb{R}^{m \times n}$, y $\mathcal{E} \in \mathbb{R}^m$ el vector de errores aleatorios, es posible describir al modelo como
\begin{equation*}
    Y = f(X) + \mathcal{E} \sim \mathcal{N}(f(X),\sigma^2 I),
\end{equation*}
donde
\begin{equation*}
    f(X) =     
    \left[
        \begin{array}{c}
        f(x_1)  \\
        ... \\
        f(x_m)
        \end{array}
    \right], 
    x_i \in \mathbb{R}^n, \forall i \in \{1,...,m\}.
\end{equation*}

Cabe recordar que la función $f$ es pensada constante, pero desconocida. De nueva cuenta, para reflejar la incertidumbre del modelador, es posible darle una distribución de probabilidad. Pero a diferencia del modelo anterior, ya no existe el parámetro $\beta$ al cual canalizarle esta incertidumbre, por lo que ahora tendrá que ser sobre toda la función. Antes de continuar, es útil tener presente la siguiente definición.

\begin{defin}
    Un \textbf{\textit{proceso gaussiano}} ($Y \in \mathbb{R}^m$), es una colección finita de \texit{m}-variables aleatorias que tienen una distribución gaussiana (normal) conjunta.
\end{defin}

Es de utilidad pensar entonces a $f(x)$ como una variable aleatoria, que refleje el desconocimiento del modelador. Particularmente se le puede asignar una distribución Normal, donde la media $m(x)$ y la covarianza $k(x,x')$ reflejen el conocimiento previo que se tenga del fenómeno de estudio. Cabe resaltar que dicha media $m(x)$ y covarianza $k(x,x')$ están en función de $x$, es decir, podrían variar de acuerdo al valor de las covariables. 

Visto de manera matricial y cometiendo un abuso de notaci\'on, dada una matriz de covariables $X \in \mathbb{R}^{m \times n}$, $f(X) \in \mathbb{R}^n$ es un vector aleatorio, que adem\'as depende de variables de entrada, por lo que \textbf{$f(X)$ es un proceso estoc\'astico}. Adem\'as, d\'andole una estructura de covarianza entre los distintos valores de las covariables, $f(X)$ se distribuye Normal Multivariada, donde su vector de medias $M(X)$ y matriz de covarianzas $K(X,X)$ reflejan el conocimiento inicial del modelador.

\newtheorem{obs}{Observaci\'on}

\begin{obs}
    De acuerdo a como se acaba de describir el vector $f(X) \in \mathbb{R}^m$, y tomando en cuenta la Definici\'on 2, además de ser un proceso estoc\'astico, \textbf{$f(X)$ es un proceso gaussiano}.
\end{obs}

% ****************************************************


\section{Inferencia sobre $f$}

\subsection{Definiciones iniciales}

Para las siguientes definiciones se supondrá que $f(x)$ es una variable aleatoria y $f(X)$ un vector aleatorio, con medias y covarianzas conocidas y finitas.

\begin{defin}
Sean $x,x' \in \mathbb{R}^n$. \\

La \textbf{función de medias de f (m\textsubscript{f})} se define como 
\begin{equation*}
    m_f: \mathbb{R}^n \rightarrow \mathbb{R} 
    \mid
    m_f(x) = \mathbb{E}[f(x)].
\end{equation*}

La \textbf{función de covarianzas de f (k\textsubscript{f})} se define como 
\begin{equation*}
    k_f: \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R} 
    \mid
    k_f(x, x') = \mathbb{E}[(f(x) - m(x))(f(x') - m(x'))].
\end{equation*}
\end{defin}

\begin{defin}
Sea $X \in \mathbb{R}^m \times \mathbb{R}^n$ y $X' \in \mathbb{R}^p \times \mathbb{R}^n$, es decir,
\begin{equation*}
    X =     
    \left[
        \begin{array}{c}
        x_1  \\
        ... \\
        x_m
        \end{array}
    \right],
    x_i \in \mathbb{R}^n, \forall i \in \{1,...,m\}.
\end{equation*}
\begin{equation*}
    X' =     
    \left[
        \begin{array}{c}
        x_1  \\
        ... \\
        x_p
        \end{array}
    \right],
    x_i \in \mathbb{R}^n, \forall i \in \{1,...,p\}.
\end{equation*}

La \textbf{función matriz de medias de f (M\textsubscript{f})} se define como
\begin{equation*}
    M_f: \mathbb{R}^m \times \mathbb{R}^n: \mathbb{R}^m
    \mid
    M_f(X) =     
    \left[
        \begin{array}{c}
        m_f(x_1)  \\
        ... \\
        m_f(x_m)
        \end{array}
    \right].
\end{equation*}

La \textbf{función matriz de covarianzas de f (K\textsubscript{f})} se define como
\begin{equation*}
    K_f: \mathbb{R}^m \times \mathbb{R}^n: \mathbb{R}^m \times \mathbb{R}^m
    \mid
    K_f(X,X') =     
    \left[
        \begin{array}{ccc}
        k_f(x_1,x_1') & ... & k_f(x_1,x_p')  \\
        ... & ... & ... \\
        k_f(x_m,x_1') & ... & k_f(x_m,x_p')
        \end{array}
    \right].
\end{equation*}
\end{defin}

Dadas estas definiciones, se puede observar que el \textit{proceso gaussiano} $f(X) \in \mathbb{R}^m$ está completamente caracterizado por su función matriz de medias $M_f(X)$ y su función matriz de covarianzas $K_f(X,X')$. Cabe resaltar que si se definen estas funciones de manera general para cualquier $X \in \mathbb{R}^{m \times n}$ que est\'e en el dominio del fen\'omeno a estudiar, en particular estar\'an definidas para cualquier matriz de datos observados o datos a predecir. Por lo tanto, la manera en que se definan estas dos funciones representar\'a el conocimiento inicial que se tiene del objeto de estudio. 

A partir de este punto, y cuando el contexto lo permita, por simplicidad de notaci\'on se omitirá el uso del subíndice $f$ en las funciones reci\'en definidas. Además, cuando se quiera referirse al proceso estoc\'astico $f(X)$ que se distribuye como un \textit{proceso gaussiano}, se har\'a con la siguiente notaci\'on:
\begin{equation*}
    f(X) \sim \mathcal{GP} (M(X),K(X,X)).
\end{equation*}

\subsection{Predicción de observaciones sin ruido}

Sea un conjunto de observaciones sin ruido, es decir, $\{(x_i,f_i)|i=1,...,m \}$, con $f_i=f(x_i)$. En otras palabras, para toda $x_i$, $y_i=f(x_i)$, sin estar sujeta a un error aleatorio. De forma matricial, se puede escribir como $\{(X,f(X))\}$, con $X \in \mathbb{R}^{m \times n}$ y $f(X) \in \mathbb{R}^{m}$. 

Por otro lado, se tiene un conjunto de covariables $X_* \in \mathbb{R}^{p \times n}$, y se desea predecir $f(X_*)$, suponiendo que sigue la misma función $f$ de los datos observados.

La distribución inicial conjunta de los datos observados $f(X)$ y los datos a predecir $f(X_*)$ es: 
\begin{equation*}
    \left[
        \begin{array}{c}
        f(X)  \\
        f(X_*) 
        \end{array}
    \right]  
    \sim \mathcal{N}  
    \left(
        \left[
            \begin{array}{c} 
            M(X) \\ 
            M(X_*) 
            \end{array}
        \right],
        \left[
            \begin{array}{cc}
            K(X,X) & K(X,X_*)  \\
            K(X_*,X) & K(X_*,X_*) 
            \end{array}
        \right]
    \right) 
\end{equation*}

Es momento oportuno para recordar algunas propiedades de la distribuci\'on Normal condicional. 

\newtheorem{prop}{Propiedad}

\begin{prop}
    Sea $X \in \mathbb{R}^p$ un vector aleatorio que tiene distribuci\'on Normal conjunta y est\'a particionado de la siguiente manera:

    \begin{equation*}
        X = 
        \left[
        \begin{array}{c}
            X_1  \\
            X_2
        \end{array}
        \right], 
        \text{ con dimensiones }
            \left[
        \begin{array}{c}
            (p-q)  \\
            q
        \end{array}
        \right],
    \end{equation*}
    
    Entonces, la media $\mu \in \mathbb{R}^p$ y varianza $\Sigma \in \mathbb{R}^{p \times p}$ de $X$ se pueden escribir
    \begin{equation*}
    \begin{aligned}
        \mu &= 
        \left[
        \begin{array}{c}
            \mu_1  \\
            \mu_2
        \end{array}
        \right], 
        \text{ dimensiones }
            \left[
        \begin{array}{c}
            (p-q)  \\
            q
        \end{array}
        \right], \\
        \Sigma &= 
        \left[
        \begin{array}{cc}
            \Sigma_{11} & \Sigma_{12}  \\
            \Sigma_{21} & \Sigma_{22}
        \end{array}
        \right], 
        \text{ dimensiones }
            \left[
        \begin{array}{cc}
            (p-q) \times (p-q)  & (p-q) \times q  \\
            q \times (p-q) & q \times q
        \end{array}
        \right].
    \end{aligned}
    \end{equation*}
    
    La distribuci\'on condicional de $X_2$, sujeta a que $X_1 = a$ es Normal con $X_2|X_1=a \sim \mathcal{N}(X_2|\bar{\mu},\bar{\Sigma})$, con
    
    \begin{equation*}
    \begin{aligned}
        \bar{\mu} &= \mu_2 + \Sigma_{2,1}\Sigma_{11}^{-1}(a-\mu_1) \\
        \bar{\Sigma} &= \Sigma_{22} - \Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12}
    \end{aligned}
    \end{equation*}
\end{prop}

De regreso al modelo, tomando en cuenta que existen datos conocidos, es posible condicionar la distribución conjunta, dadas esas observaciones. Utilizando las propiedades de la distribución Normal condicional, se obtiene que:
\begin{equation*}
    f(X_*)|f(X) 
    \sim \mathcal{N}
    (\bar{M}(X,X_*),\bar{K}(X,X_*)),
\end{equation*}
con
\begin{equation*}
\begin{aligned}
    \bar{M}(X,X_*) &= M(X_*) + K(X_*,X)K(X,X)^{-1}(f(X) - M(X)), \\
    \bar{K}(X,X_*) &= K(X_*,X_*) - K(X_*,X)K(X,X)^{-1}K(X,X_*).
\end{aligned}
\end{equation*}

\begin{obs}
    $f(X_*)|f(X)$ es una colección finita de p-variables aleatorias que tienen una distribuci\'on Normal conjunta, por lo tanto, $\mathbf{f(X_*)|f(X)}$ \textbf{es un proceso gaussiano}. 
\end{obs}

Para confirmar que no existe ruido en las observaciones, es posible sustituir $X_* = X$ y ver los posible valores para $f(X_*)$.
\begin{equation*}
\begin{aligned}
    \mathbb{E}[f(X_*)|f(X)] 
    &= \bar{M}(X,X_*) \\
    &= \bar{M}(X,X) \\
    &= M(X) + K(X,X)K(X,X)^{-1}(f(X) - M(X)) \\
    &= M(X) + f(X) - M(X) \\
    &= f(X) ,
\end{aligned}
\end{equation*}
\begin{equation*}
\begin{aligned}
    Var(f(X_*)|f(X)) 
    &= \bar{K}(X,X_*) \\
    &= \bar{K}(X,X) \\
    &= K(X,X) - K(X,X)K(X,X)^{-1}K(X,X) \\
    &= K(X,X) - K(X,X) \\
    &= 0.
\end{aligned}
\end{equation*}

Es decir, si $X_* = X$, $f(X_*) \sim \mathcal{N}(f(X),0)$. En otras palabras, la media es el vector de valores ya obtenidos $f(X)$ y varianza 0, por lo que se cumple que para cualquier $X$, $f(X)$ tendría siempre un único valor.

\subsection{Predicción de observaciones con ruido}

Ahora se supone un conjunto de observaciones con ruido, es decir, $\{(x_i,y_i)|i=1,...,m \}$, con $y_i=f(x_i) + \epsilon$, donde $\epsilon \sim \mathcal{N}(0,\sigma^2)$ y $\sigma^2 > 0$. Con notación matricial se puede describir a este conjunto como $\{(X,Y)\}$, con $X \in \mathbb{R}^{m \times n}$ y $Y \in \mathbb{R}^{m}$, y el vector de errores aleatorios es $\mathcal{E} \sim \mathcal{N}(0,\sigma^2I)$. Así, se tiene que
\begin{equation*}
    Y = f(X) + \mathcal{E}.
\end{equation*}

Es posible observar que al ser suma de dos Normales, la distribuci\'on inicial de $Y$ ser\'a Normal. Por lo tanto, 
\begin{equation*}
    Y = f(X) + \mathcal{E} \sim \mathcal{N}(M(X), K(X,X) + \sigma^2I).
\end{equation*}

\begin{obs}
    Y es una colección finita de m-variables aleatorias que tienen una distribuci\'on Normal conjunta, por lo tanto, \textbf{$Y$ es un proceso gaussiano}.
\end{obs}

A partir de este punto, en esta sección se supondrá a \textbf{$\sigma^2$ como constante y conocida}, y la atención principal estará sobre la función $f$.

Ahora se piensa en un conjunto de covariables $X_* \in \mathbb{R}^{p \times n}$, y se busca predecir $f(X_*)$, suponiendo que sigue la misma función $f$ de los datos observados. La distribución inicial conjunta de los datos observados $Y$ y los datos a predecir $f(X_*)$ es: 
\begin{equation*}
    \left[
        \begin{array}{c}
        Y \\
        f(X_*)
        \end{array}
    \right]  
    \sim \mathcal{N}  
    \left(
        \left[
            \begin{array}{c} 
            M(X) \\ 
            M(X_*) 
            \end{array}
        \right],
        \left[
            \begin{array}{cc}
            K(X,X) + \sigma^2 I_{m} & K(X,X_*)  \\
            K(X_*,X) & K(X_*,X_*)
            \end{array}
        \right]
    \right), 
\end{equation*}
donde $I_m$ es la matrz identidad de dimensi\'on $m$.

Considerando que ya se cuenta con datos conocidos, se toma la distribuci\'on condicional de la Normal y se obtiene que:
\begin{equation*}
    f(X_*)|Y
    \sim \mathcal{N}
    (\bar{M}(X,X_*,\sigma^2),\bar{K}(X,X_*,\sigma^2)),
\end{equation*}
con
\begin{equation*}
\begin{aligned}
    \bar{M}(X,X_*,\sigma^2) &= M(X_*) + K(X_*,X)(K(X,X) + \sigma^2I_m)^{-1}(Y - M(X)) \\
    \bar{K}(X,X_*,\sigma^2) &= K(X_*,X_*) - K(X_*,X)(K(X,X) + \sigma^2I_m)^{-1}K(X,X_*).
\end{aligned}
\end{equation*}

\begin{obs}
    $f(X_*)|Y$ es una colección finita de p-variables aleatorias que tienen una distribuci\'on Normal conjunta, por lo tanto,
    $\mathbf{f(X_*)|Y}$ \textbf{ es un proceso gaussiano}. 
\end{obs}

Es posible observar que aunque $X_* = X$, no necesariamente se cumple que $f(X_*)|Y = Y$. En primer lugar, porque $\bar{K}(X,X,\sigma^2) \neq 0$, debido al efecto de $\sigma^2$. En segundo lugar, y de nueva cuenta por causa de $\sigma^2$, $\bar{M}(X,X,\sigma^2) \neq Y$.

\section{Varianza}

\subsection{Funciones de covarianza}

Hasta el momento, no se han descrito las caracter\'isticas de la funci\'on de covarianzas de $f$ $(k_f)$. Se empezar\'a por recordar que la \textbf{función de covarianzas de f (k\textsubscript{f})} se define como 
\begin{equation*}
    k_f: \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R} 
    \mid
    k_f(x, x') = \mathbb{E}[(f(x) - m_f(x))(f(x') - m_f(x'))],
\end{equation*}
donde $f(x)$ es una variable aleatoria que se distribuye Normal con media $m_f(x)$. Por lo tanto, se deduce que 
\begin{equation*}
    Cov(f(x),f(x')) = k_f(x,x').
\end{equation*}

Cabe resaltar que $k_f$ no es una \textit{covarianza} en general, ni cumple con todas las propiedades, sino \'unicamente describe la covarianza entre dos vectores aleatorios $f(x)$ y $f(x')$, con la misma $f$, sin la intervenci\'on, por ejemplo, de constantes. Para explicar de mejor manera este punto, se da el siguiente ejemplo:
\begin{equation*}
\begin{aligned}
    Cov(af(x) + f(x'), f(x')) &=
    Cov(af(x), f(x')) + Cov(f(x), f(x'))\\
     &= a \times Cov(f(x), f(x')) +  Cov(f(x'), f(x')) \\
     &= a \times k_f(x,x') + k_f(x',x')
\end{aligned}
\end{equation*}

En este orden de ideas, las propiedades que $k_f(x,x')$ tiene que cumplir son
\begin{equation*}
\begin{aligned}
    k_f(x,x') &= k_f(x',x) \text{ (simetr\'ia),} \\
    k_f(x,x) &= Var(f(x)) \geq 0.
\end{aligned}
\end{equation*}

Si bien es cierto que dadas esas restricciones hay una variedad muy grande de funciones con las que se puede describir $k_f(x,x')$, por practicidad, y tomando en cuenta que es un supuesto sensato para la mayor\'ia de los casos, es com\'un describir a la funci\'on $k_f$ en relaci\'on a la distancia entre $x$ y $x'$, $\norm{x,x'}_p$. Es decir, $k_f(x,x') = k_f(\norm{x,x'}_p)$. A este tipo de funciones de covarianza se les denomina \textbf{estacionarias}.

Adem\'as, esta relaci\'on entre covarianza y distancia suele ser inversa, es decir, entre menor sea la distancia, mayor ser\'a la covarianza, y viceversa. De esta manera, para valores $x \approx x'$, se obtendr\'a que $f(x) \approx f(x')$ en la mayor\'ia de los casos, lo que tiene cierto supuesto impl\'icito de que $f$ es una funci\'on continua.

Un ejemplo de este tipo de funciones son las $\mathbf{\gamma}$\textbf{\textit{-exponencial}}, mismas que se definen de la siguiente manera:
\begin{equation*}
    k(x,x') = 
    k(\norm{x,x'}_\gamma;\gamma,\lambda) = 
    exp\left(-\frac{1}{\gamma}
    \left(\frac{\norm{x,x'}_\gamma}{\lambda}\right)^\gamma
    \right),
\end{equation*}
donde $\lambda$ es un par\'ametro de rango. 

Las de uso m\'as com\'un suelen ser la $1$ y $2$\textit{-exponencial}. Ambas tienen la ventaja de ser continuas, pero la $2$\textit{-exponencial} tiene adem\'as la peculiaridad de ser infinitamente diferenciable y, por lo tanto, es suave.

El siguiente ejemplo de funciones estacionarias es la \textbf{\textit{clase de Matérn}}, descrita como
\begin{equation*}
    k(x,x') = k(\norm{x,x'}_1;\nu,\lambda) = 
    \frac{2^{1-\nu}}{\Gamma(\nu)}
    \left(\frac{\sqrt{2\nu}\norm{x,x'}_1}{\lambda}\right)^\nu
    K_{\nu}
    \left(\frac{\sqrt{2\nu}\norm{x,x'}_1}{\lambda}\right)^\nu,
\end{equation*}
donde $K_{\nu}$ es la funci\'on modificada de Bessel y $\Gamma(.)$ es la funci\'on \textit{gamma}. Los casos m\'as utilizados son
\begin{equation*}
\begin{aligned}
    k\left(\norm{x,x'}_1;\nu = \frac{3}{2},\lambda\right) &= 
    \left(1 + \frac{\sqrt{3}\norm{x,x'}_1}{\lambda}\right)
    exp\left(-\frac{\sqrt{3}\norm{x,x'}_1}{\lambda}\right), \\
    k\left(\norm{x,x'}_1;\nu = \frac{5}{2},\lambda\right) &= 
    \left(1 + \frac{\sqrt{3}\norm{x,x'}_1}{\lambda} + \frac{5\norm{x,x'}_1^2}{3\lambda^2}\right)
    exp\left(-\frac{\sqrt{5}\norm{x,x'}_1}{\lambda}\right). \\
\end{aligned}
\end{equation*}

Otra posible funci\'on de covarianza es la \textbf{\textit{racional cudr\'atica}}, caracterizada como 
\begin{equation*}
    k(x,x') = k(\norm{x,x'}_2;\alpha,\lambda) = 
    \left(1 + \frac{\norm{x,x'}_2^2}{2\alpha \lambda^2}\right)^{-\alpha},
\end{equation*}
con $\alpha,\lambda > 0$.

Existen otro tipo de funciones estacionarias que no guardan una relaci\'on inversa entre distancia y covarianza, sino que a cierta distancia aumenta la covarianza, y esto sucede de forma c\'iclica. En otras palabras, este tipo de funciones capturan un componente \textbf{estacional}, normalmente usado en series de tiempo. De esta manera, y siendo $t$ la covariable del tiempo, es posible pensar en una funci\'on de la forma
\begin{equation*}
    k(x,x',t,t';E) = \bar{k}(x,x') + \delta_{\{(|t'-t| \text{ mod } E) = 0\}},
\end{equation*}
donde $\bar{k}$ es alguna de las funciones estacionarias antes mencionadas, $\delta$ es la \textit{delta de Kroenecker} y $E$ es el periodo de estacionalidad. Por ejemplo, $E = 12$ para una serie mensual.

Si se desea suavizar esta componente de estacionalidad para que no sea \'unicamente puntual, es posible describir la covarianza con una funci\'on como la siguiente:
\begin{equation*}
    k(x,x',t,t';E,\lambda) = 
    \bar{k}(x,x') + 
    exp\left(-\frac{1}{\lambda^2}\frac{E}{\pi}sin^2\left(\frac{\pi}{E}|t'-t|\right)\right).
\end{equation*}

\subsection{Varianza del error aleatorio}

Una vez dicho todo lo anterior, el \'unico pendiente restante es dejar de suponer a $\sigma^2$ (la varianza del error aleatorio $\varepsilon$) como una constante conocida. Como ya se mencion\'o anteriormente, si bien se piensa en ella como constante, es posible reflejar la incertidumbre del modelador respecto al valor verdadero con una distribuci\'on de probabilidad. Es decir, si H son las creencias o la informaci\'on previa con la que se cuenta, entonces
\begin{equation*}
    \sigma^2 \sim P(\sigma^2 | H).
\end{equation*}

Es claro que esta distribuci\'on tiene que tener soporte en alg\'un subconjunto de $\mathbb{R^+}$, por lo que la distribuci\'on Gamma o Gamma Inversa son las com\'unmente utilizadas. Suponiendo que es la primera, el conocimiento de $H$ se tendr\'a que traducir en los par\'ametros $\alpha$ y $\beta$.

As\'i, el modelo de Procesos Gaussianos queda especificado como
\begin{equation*}
\begin{aligned}
    y - f(x) | f(x), \sigma^2 &\sim \mathcal{N}(0,\sigma^2) \\
    f(x) &\sim \mathcal{GP}(m(x),k(x,x)) \\
    \sigma^2 &\sim Gamma(\alpha,\beta).
\end{aligned}
\end{equation*}

\newpage