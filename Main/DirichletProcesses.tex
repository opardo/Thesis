\chapter[Procesos de Dirichlet]{Procesos de Dirichlet\raisebox{.3\baselineskip}{\normalsize\footnotemark}}\footnotetext{Las ideas de este capítulo son retomadas de \cite{Yee_DirProc}.}

\section{Motivaci\'on}
Un Proceso de Dirichlet, visto de manera general, es una distribuci\'on sobre distribuciones. Es decir, cada realizaci\'on de él es en sí misma una distribuci\'on de probabilidad. Adem\'as, cada una de esas distribuciones ser\'a no param\'etrica, debido a que no ser\'a posible describirla con un n\'umero finito de par\'ametros.

Emplear distribuciones de este tipo permite combatir, por un lado, el \textit{subajuste}, debido a que cualquier distribuci\'on se puede representar de manera no param\'etrica. Por otro lado, combate al \textit{sobreajuste} utilizando un enfoque bayesiano para calcular la probabilidad posterior, dando como distribuci\'on inicial a aquella que se percibe como la m\'as factible.

En el caso particular de esta tesis y de su misi\'on de encontrar un modelo bayesiano y no param\'etrico para la \textit{regresi\'on sobre cuantiles}, los Procesos de Dirichlet ser\'an utilizados para ajustar la distribuci\'on del error aleatorio $\varepsilon$.


\section{Inferencia}

\subsection{Definici\'on formal}

Antes de revisar la definici\'on formal de los Procesos de Dirichlet, es conveniente recordar la definici\'on de la distribuci\'on de Dirichlet.

\begin{defin}
    Se dice que un vector aleatorio $x \in \mathbb{R}^p$ se distribuye de acuerdo a la \textbf{distribuci\'on de Dirichlet}  $\mathbf{(x \sim Dir(\alpha))}$ con vector de par\'ametros $\alpha$, espec\'ificamente,
    \begin{equation*}
        x = 
        \left(\begin{array}{c}
            x_1  \\
            \cdots \\
            x_p
        \end{array}\right),
        \qquad
        \alpha = 
        \left(\begin{array}{c}
            \alpha_1  \\
            \cdots \\
            \alpha_p
        \end{array}\right),
    \end{equation*}
    para los cuales se cumplen las restricciones
    \begin{equation*}
    \begin{aligned}
        x_i > 0, \forall i &\in \{1,...,p\} \\
        \sum_{i=1}^p x_i &= 1 \\
        \alpha_i > 0, \forall i &\in \{1,...,p\},
    \end{aligned}
    \end{equation*}
    si su funci\'on de densidad  es
    \begin{equation*}
        f(x|\alpha) = 
        \frac {1}{\mathrm {B} (\alpha)}
        \prod _{i=1}^{p}x_{i}^{\alpha _{i}-1},
    \end{equation*}
    donde $\mathrm{B}$ es la funci\'on Beta multivariada, y puede ser expresada en t\'erminos de la funci\'on $\Gamma$ como 
    \begin{equation*}
       \mathrm{B}(\alpha)=
       \frac {\prod _{i=1}^{p}\Gamma (\alpha _{i})}
       {\Gamma \left(\sum _{i=1}^{p}\alpha _{i}\right)},
       \qquad 
       \alpha =(\alpha _1,\cdots ,\alpha _p). 
    \end{equation*}
    
    La esperanza y varianza de cada $x_i$ son los siguientes:
    
    \begin{equation*}
    \begin{aligned}
        \mathbb{E}[x_i] &= \frac{\alpha_i}{\sum_{k=1}^p \alpha_k} \\
        Var(x_i) &= \frac
        {\alpha_i \left( \sum_{k=1}^p \alpha_k - \alpha_i \right)}
        {\left( \sum_{k=1}^p \alpha_k \right)^2 (\left( \sum_{k=1}^p \alpha_k + 1 \right)}
    \end{aligned}
    \end{equation*}
    
\end{defin}
Es com\'un que esta distribuci\'on sea usada como la inicial conjugada de la distribuci\'on multinomial, debido a que el vector $x$ tiene las mismas propiedades de una distribuci\'on de probabilidad discreta (elementos positivos y que en conjunto suman 1).

Retomando el tema central, en t\'erminos generales, para que una distribuci\'on de probabilidad $G$ se distribuya de acuerdo a un Proceso de Dirichlet, sus distribuciones marginales tienen que tener una distribuci\'on Dirichlet. A continuaci\'on de enuncia una definici\'on m\'as detallada.

\begin{defin}
    Sean $G$ y $H$ dos distribuciones cuyo soporte es el conjunto $\Theta$ y sea $\alpha \in \mathbb{R}^+$. Entonces, si se toma una partici\'on finita cualquiera $A_1,...,A_r$ del conjunto $\Theta$, el vector $(G(A_1),...,G(A_r))$ es aleatorio, porque $G$ tambi\'en lo es.
    
    Se dice que $G$ se distribuye de acuerdo a un \textbf{Proceso de Dirichlet} $\mathbf{(G \sim DP(\alpha,H))}$, con distribuci\'on media $H$ y par\'ametro de concentraci\'on $\alpha$, si
    \begin{equation*}
        (G(A_1),...,G(A_r)) \sim Dir(\alpha H(A_1),...,\alpha H(A_r)), 
    \end{equation*}
    para cualquier partici\'on finita $A_1,...,A_r$ del conjunto $\Theta$.
\end{defin}

Es momento de analizar el papel que juegan los par\'ametros. Sea $Ai \subset \Theta$, uno de los elementos de la partici\'on anterior, y recordando las propiedades de la distribuci\'on de Dirichlet, entonces
\begin{equation*}
\begin{aligned}
    E[G(A_i)] 
    &= \frac{\alpha H(A_i)}{\sum_{k=1}^p \alpha H(A_k)} \\
    &= H(A_i) \\
\end{aligned}
\end{equation*}

\begin{equation*}
\begin{aligned}
    Var(G(A_i)) 
    &= \frac{\alpha H(A_i)\left(\sum_{k=1}^p(\alpha H(A_k)) - \alpha H(A_i)\right)}
       {\left(\sum_{k=1}^p \alpha H(A_k)\right)^2\left(\sum_{k=1}^p(\alpha H(A_k)) + 1\right)} \\
    &= \frac{\alpha^2 [H(A_i)(1 - H(A_i))]}
       {\alpha^2 (1)^2(\alpha + 1)} \\
    &= \frac{H(A_i)(1 - H(A_i))}
       {\alpha + 1}.
\end{aligned}
\end{equation*}

En este orden de ideas, es posible darse cuenta que la distribuci\'on $H$ representa la \textit{distribuci\'on media} del Proceso de Dirichlet. Por otro lado, el par\'ametro $\alpha$ tiene una relaci\'on inversa con la varianza. As\'i, a una mayor $\alpha$, corresponde una menor varianza del Proceso de Dirichlet, y, por lo tanto, una mayor concentraci\'on respecto a la distribuci\'on media $H$. 

Siguiendo la secuencia l\'ogica, si $\alpha \rightarrow \infty$, entonces $G(A_i) \rightarrow H(A_i)$ para cualquier elemento $A_i$ de la partici\'on. Es decir, $G \rightarrow H$ en distribuci\'on. Sin embargo, cabe aclarar que esto no es lo mismo que $G \rightarrow H$. Por un lado, $H$ puede ser una distribuci\'on de probabilidad continua, mientras que, como se ver\'a m\'as adelante, $G$ puede arrojar dos muestras iguales con probabilidad mayor a 0, por lo que es una distribuci\'on discreta.

\subsection{Distribuci\'on posterior}

Sea $G \sim DP(\alpha,H)$. Dado que $G$ es (aunque aleatoria) una distribuci\'on, es posible obtener realizaciones de ella. Sean $\phi_1,..., \phi_n$ una secuencia de realizaciones independientes de $G$, que toman valores dentro de su soporte $\Theta$. Sea de nuevo $A_1,...,A_r$ una partici\'on finita cualquiera del conjunto $\Theta$, y sea $n_k = |\{i: \phi_i \in A_k\}|$ el n\'umero de valores observados dentro del conjunto $A_k$. Por la propiedad conjugada entre la distribuci\'on de Dirichlet y la distribuci\'on Multinomial, se obtiene que
\begin{equation*}
   (G(A_1),...,G(A_r))|\phi_1,...,\phi_n \sim Dir(\alpha H(A_1) + n_1,...,\alpha H(A_r) + n_r). 
\end{equation*}

Es momento de analizar el par\'ametro $\alpha H(A_k) + n_k$ de la distribuci\'on de Dirichlet, correspondiente a $G(A_k|\phi_1,...,\phi_n)$, donde $k \in \{1,...,r\}$. Pero antes, es importante observar que es posible reescribir $n_k = \sum_{i=1}^n \delta_i(A_k)$, donde $\delta_i(A_k) = 1$ si $\phi_i \in A_k$, y $0$ en cualquier otro caso. 
\begin{equation*}
\begin{aligned}
    \alpha H(A_k) + n_k 
    &= \alpha H(A_k) + \sum_{i=1}^n \delta_i(A_k) \\
    &= (\alpha + n)
    \left[
        \frac{\alpha \times H(A_k) + n \times \frac{\sum_{i=1}^n \delta_i(A_k)}{n}}{\alpha + n}
    \right] \\
    &= \bar{\alpha} \bar{H}(A_k),
\end{aligned}
\end{equation*}
donde
\begin{equation*}
\begin{aligned}
    \bar{\alpha} &= \alpha + n \\
    \bar{H}(A_k) &=  
        \left(\frac{\alpha}{\alpha + n}\right)H(A_k) + 
        \left(\frac{n}{\alpha + n}\right)\frac{\sum_{i=1}^n \delta_i(A_k)}{n}.
\end{aligned}
\end{equation*}

Por lo tanto, $G|\phi_1,...,\phi_n \sim DP(\bar{\alpha},\bar{H})$. Es decir, la probabilidad posterior de $G$ sigue distribuy\'endose mediante un Proceso de Dirichlet, con par\'ametros actualizados. Asimismo, se puede interpretar a la distribuci\'on media posterior $\bar{H}$ como una mezcla entre la distribuci\'on media inicial, con peso proporcional al par\'ametro de concentraci\'on inicial $\alpha$, y la distribuci\'on emp\'irica de los datos, con peso proporcional al n\'umero de observaciones $n$. Por otro lado, el par\'ametro de concentaci\'on posterior es estrictamente m\'as grande que el inicial, por lo que a medida que crecen las $n$ observaciones, la varianza del Proceso de Dirichlet se reduce y el Proceso se concentra alrededor de la distribuci\'on emp\'rica. 

\subsection{Distribuci\'on predictiva}

Continuando con la idea de la secci\'on anterior de que ya se conoce el valor de $\phi_i,...,\phi_n$ realizaciones provenientes de la distribuci\'on aleatoria $G$, se desea hacer predicci\'on de la observaci\'on $\phi_{n+1}$, condicionada a los valores observados. As\'i,
\begin{equation*}
\begin{aligned}
   P(\phi_{n+1} \in A_k|\phi_1,...,\phi_n)
   &= \int P(\phi_{n+1} \in A_k|G) P(G|\phi_1,...,\phi_n) dG \\ 
   &= \int G(A_k) P(G|\phi_1,...,\phi_n) dG \\ 
   &= \mathbb{E}[G(A_k)|\phi_1,...,\phi_n] \\
   &= \bar{H}(A_k),
\end{aligned}    
\end{equation*}
es decir, 
\begin{equation*}
    \phi_{n+1}|\phi_1,...,\phi_n \sim 
    \left(\frac{\alpha}{\alpha + n}\right)H(\phi_{n+1}) + 
    \left(\frac{n}{\alpha + n}\right)\frac{\sum_{i=1}^n \delta_i(\phi_{n+1})}{n}.
\end{equation*}

Cabe resaltar que dicha distribuci\'on predictiva tiene puntos de masa localizados en $\phi_1,...,\phi_n$. Esto significa que la probabilidad de que $\phi_{n+1}$ tome un valor que ya ha sido observado es mayor a $0$, independientemente de la forma de $H$. Yendo a\'un m\'as all\'a, es posible darse cuenta que si se obtienen realizaciones infinitas de $G$, cualquier valor obtenido ser\'a repetido eventualmente, con probabilidad igual a $1$. Por lo tanto, $G$ es una distribuci\'on discreta.

\section{Problemas equivalentes y aplicaciones}

En esta secci\'on se revisar\'a la equivalencia entre los Procesos de Dirichlet y otros problemas famosos en la literatura, lo que facilitar\'a el entendimiento de la intiuici\'on que hay detr\'as, as\'i como la resoluci\'on y demostraci\'on de algunas propiedades pendientes.

\subsection{Esquema de urna de Blackwell-MacQueen}

Sea $\Theta$ un conjunto (finito o infinito) cuyos elementos son colores distintos al negro y al blanco, y donde cada color es distinto entre s\'i. Existe una m\'aquina llamada $H$ que cada que se le oprime \textit{Play} arroja de manera aleatoria una pelota con algún color perteneciente al conjunto $\Theta$, siguiendo una regla de probabilidad dada previamente. Se tienen 2 urnas: una llamada \textit{probabilidades}, que contiene $\alpha$ bolas negras. Otra llamada \textit{resultados}, que en un principio se encuentra vac\'ia. 

Se oprime \textit{Play} a la m\'aquina, y se obtiene una pelota, la cual se arroja a la urna \textit{resultados}. A $\phi_1$ se le aginar\'a el color de dicha pelota. Posteriormente se añade una pelota de color blanca a la urna \textit{probabilidades} y se pasa a la segunda ronda.  

Las siguientes rondas, por ejemplo la ronda $n+1$, comienza tomando al azar una pelota de la urna \textit{probabilidades}. Si el color de la pelota es negra (probabilidad proporcional a $\alpha$), se obtiene una nueva pelota de la m\'aquina $H$ y se repite lo sucedido en la primera ronda, incluyendo al asignar el color de la pelota a $\phi_{n+1}$. Si es blanca, se toma al azar una pelota de la urna \textit{resultados}, se asigna el color de esa pelota a $\phi_{n+1}$ y se regresa a la urna de \textit{resultados} esa misma pelota, as\'i como una nueva pintada del mismo color. En ambos casos, después de hacer lo antes mencionado, se introduce una nueva pelota blanca a la urna \textit{probabilidades} y se pasa a la siguiente ronda. 

Así, despu\'es de $n$ rondas, se obtiene la secuencia $\phi_1,...,\phi_n$. Es importante notar que cada $\phi_{k+1}$ es una variable aleatoria que depende de las $k$ anteriores, y cuya distribuci\'on es
\begin{equation*}
    \phi_{k+1}|\phi_1,...,\phi_k \sim 
    \left(\frac{\alpha}{\alpha + k}\right)H(\phi_{k+1}) + 
    \left(\frac{k}{\alpha + k}\right)\frac{\sum_{i=1}^k \delta_i(\phi_{k+1})}{k}.
\end{equation*}

La distribuci\'on conjunta de $\phi_1,...,\phi_n$ se puede obtener como
\begin{equation*}
    P(\phi_1,...,\phi_n) = 
    P(\phi_1)
    \prod_{i=2}^n
    P(\phi_i|\phi_1,...,\phi_{i-1})
\end{equation*}

Antes de continuar, es importante repasar una definici\'on. 

\begin{defin}
    Sea $\phi_1,...,\phi_n$, una secuencia de $n$ variables aleatorias, cuya distribuci\'on de probabilidad conjunta est\'a dada por $P(\phi_1,...,\phi_n)$. Sea $\psi$ una funci\'on biyectiva, que va de $\{1,...,n\} \rightarrow \{1,...,n\}$, es decir, una funci\'on que crea una permutaci\'on del conjunto $\{1,...,n\}$.  
    Entonces, se dice que $\phi_1,...,\phi_n$ es una \textbf{secuencia aleatoria infinitamente intercambiable} si se cumple que 
    \begin{equation*}
        P(\phi_1,...,\phi_n) = P(\phi_{\psi(1)},...,\phi_{\psi(n)}),
    \end{equation*}
    para cualquier permutaci\'on $\psi$.
\end{defin}

Regresando al juego de urnas, es importante observar que si bien $\phi_{k+1}$ es dependiente de las $k$ observaciones anteriores, esta dependencia s\'olo se da en t\'erminos de los valores observados previamente y la frecuencia de dichas observaciones, pero el orden en que hayan sido obtenidos no es relevante. Por lo tanto, es posible afirmar que $\phi_1,...,\phi_n$ es una secuencia aleatoria infinitamente intercambiable. Dicho esto, es conveniente recordar el \textbf{\textit{Teorema de represesentaci\'on general de de Finetti}}.\footnote{Una demostraci\'on de este teorema puede ser encontrada en \cite{Schervish_TheoryStats}.}

\newtheorem{theorem}{Teorema}

\begin{theorem}
    Sea $\phi_1, ...,\phi_n$ una secuencia aleatoria infinitamente intercambiable de valores reales. Entonces existe una distribuci\'on de probabilidad $G$ sobre $\mathcal{F}$, el espacio de todas las distribuciones, de forma que la probabilidad conjunta de $\phi_1, ...,\phi_n$ se puede expresar como
    \begin{equation*}
        P(\phi_1, ...,\phi_n) =
        \int_{\mathcal{F}}\left[\prod_{k=1}^n G(\phi_k)\right]dP(G),
    \end{equation*}
    con
    \begin{equation*}
        P(G) = \lim_{n \to \infty} P(G_n),
    \end{equation*}
    donde $P(G_n)$ es una funci\'on de distribuci\'on evaluada en la funci\'on de distribuci\'on emp\'irica definida por
    \begin{equation*}
        G_n = \frac{1}{n} \sum_{i=1}^n I(y_i \leq y).
    \end{equation*}
    En otras palabras, el Teorema de de Finetti dice que existe una distribuci\'on $G$ tal que $\phi_1, ...,\phi_n$ son condicionalmente independientes, dada dicha $G$. A su vez dicha $G$ es aleatoria y sigue una distribuci\'on $P(G)$.
\end{theorem}

Una vez dicho esto, y sean $\phi_1,...,\phi_n$ una secuencia de colores obtenida con la rutina de esta secci\'on, es posible darse cuenta que cada $\phi_k \sim G$. Adem\'as $P(G) = DP(\alpha,H)$, seg\'un lo visto en la secci\'on anterior. Con esto, queda demostrada la existencia de los Procesos de Dirichlet.

\subsection{Proceso estoc\'astico del restaurante chino}

Sean $\phi_1,...,\phi_n$ una secuencia de realizaciones de $G$, con $G \sim DP(\alpha,H)$. Recordando lo mencionado anteriormente, cada valor obtenido tiene una probabilidad mayor a $0$ de ser repetido en una nueva observaci\'on. 

Sean $\phi_1^*,...,\phi_m^*$ los $m$ valores \'unicos observados, y sea $n_k^*$ sea el n\'umero de veces que se repite cada valor $\phi_k^*$. Entonces, la distribuci\'on predictiva se puede reescribir como
\begin{equation*}
    \phi_{n+1}|\phi_1,...,\phi_n \sim 
    \left(\frac{\alpha}{\alpha + n}\right)H(\phi_{n+1}) + 
    \left(\frac{n}{\alpha + n}\right)\frac{\sum_{k=1}^m n_k^*\delta_{\phi_k^*}(\phi_{n+1})}{n}.
\end{equation*}

A partir de este momento se definir\'a como el \textit{cluster} $\Phi_k^*$ al conjunto cuyos elementos son todos los $\phi_i 's$ id\'enticos que tomen el valor $\phi_k^*$. Es inmediato observar que la probabilidad de que una nueva observaci\'on $\phi_{n+1}$ se ubique dentro del \textit{cluster} $\Phi_k^*$ es proporcional a $n_k^*$. Es decir, se da el fen\'omeno de \textit{los ricos se vuelven m\'as ricos}, ya que entre mayor sea el n\'umero de elementos de un \textit{cluster}, mayor ser\'a la probabilidad de que una nueva observaci\'on sea parte de \'el.

As\'i, queda inducida una partici\'on sobre el conjunto $N = \{1,...,n\}$, debido a que cada uno de dichos n\'umeros naturales pertenece a un, y s\'olo un, $\Phi_k^*$. Adem\'as, el \textit{cluster} al que pertenecer\'a cada uno es aleatorio, por lo que la partici\'on inducida tambi\'en es aleatoria, y encapsula todas las propiedades de los Procesos de Dirichlet.

Para ver c\'omo sucede esto, \'unicamente hay invertir el proceso generador. En este caso, primero se obtiene de manera aleatoria una partici\'on del conjunto $N$ en \textit{clusters} $\Phi_1^*,...,\Phi_m^*$. Despu\'es, para cada $\Phi_k^*$ se encuentra su respectivo valor mediante una realizaci\'on de $\phi_k^* \sim H$, y se asigna $\phi_i = \phi_k^*$, para toda $i \in \Phi_k^*$.

La distribuci\'on sobre las particiones ha sido nombrada el \textit{Proceso estoc\'astico del restaurante chino}, y es un problema que ha sido estudiado de manera independiente a los Procesos de Dirichlet, siendo descubierta posteriormente su equivalencia. Su nombre lo toma de la siguiente met\'afora.

Se supone un restaurante con infinito n\'umero de mesas e infinitas sillas en cada una de ellas. El primero consumidor entra y se sienta en la mesa $\Phi_1^*$. El segundo entra y con probabilidad $\frac{1}{\alpha + 1}$ se sienta en la misma mesa $\Phi_1^*$ del consumidor anterior, mientras que con probabilidad $\frac{\alpha}{\alpha+1}$ se sienta en una nueva mesa $\Phi_2^*$. 

En general, despu\'es de que han entrado $n$ personas, han sido ocupadas $m$ mesas. Sea $n_k^*$ el n\'umero de personas que est\'an sentadas en la mesa $\Phi_k^*$, una nueva persona $n+1$ se sienta en una mesa con las siguientes probabilidades:
\begin{equation*}
\begin{aligned}
   P(n+1 \in \Phi_{m+1}^*) &= \frac{\alpha}{\alpha + n},\\
    P(n+1 \in \Phi_{k}^*) &= \frac{n_k^*}{\alpha + n},
\end{aligned}
\end{equation*}
siendo $\Phi_{m+1}^*$ una mesa que a\'un no ha sido ocupada.

Para conectar esta met\'afora con los Procesos de Dirichlet, se podr\'ia pensar que todos los integrantes de cada mesa comer\'an el mismo platillo, mismo que ser\'ia elegido aleatoriamente mediante la distribuci\'on H, entre un infinito n\'umero de platillos.

\subsection{Proceso estoc\'astico de rompimiento de un palo}
Es importante recordar que una realizaci\'on $G$ de un Proceso de Dirichlet es una distribuci\'on discreta con probabilidad $1$, debido a que toda muestra tiene probabilidad mayor a $0$ de ser repetida. Por lo tanto, se puede expresar a $G$ como una suma de centros de masa, de la siguiente manera:
\begin{equation*}
\begin{aligned}
   \phi_k^* &\sim H, \\
   G(\phi) &= \sum_{k=1}^\infty \pi_k \delta_{\phi_k^*}(\phi),
\end{aligned}
\end{equation*}
siendo $\pi_k$ la probabilidad de ocurrencia de $\phi_k$.

Dicha probabilidad de ocurrencia ser\'a generada con la siguiente met\'afora. Se piensa un palo de longitud 1. Se genera una n\'umero aleatorio $\beta_1 \sim Beta(1,\alpha)$, mismo que estar\'a en el intervalo $(0,1)$. Esa ser\'a la magnitud del pedazo que ser\'a separado del palo de longitud 1, y le ser\'a asignado a $\pi_1 = \beta_1$. As\'i, quedar\'a un palo de magnitud $(1-\beta_1)$ a repartir. Posteriormente se vuelve a generar un n\'umero aleatorio $\beta_2 \sim Beta(1,\alpha)$, que representar\'a la proporci\'on del palo restante que le ser\'a asignada a $\pi_2$. Es decir, $\pi_2 = \beta_2(1-\beta_1)$. En general, para $k \geq 2$,
\begin{equation*}
\begin{aligned}
   \beta_k &\sim Beta(1,\alpha),\\
   \pi_k &= \beta_k \prod_{i=1}^{k-1}(1 - \beta_i).
\end{aligned}
\end{equation*}
Dada su construcci\'on, es inmediato darse cuenta que $\sum_{k=1}^\infty \pi_k = 1$. Algunas ocasiones se nombra a esta distribuci\'on $\pi \sim GEM(\alpha)$, en honor a Griffiths, Engen y McCloskey.

\subsection{Modelo de mezclas infinitas de Dirichlet}

Sean $\{y_1,...,y_n\}$ un conjunto de observaciones con distribuci\'on $F$, condicionalmente independientes, y que se suponen vienen del \textit{Modelo de mezclas de Dirichlet}:
\begin{equation*}
\begin{aligned}
   y_i | \phi_i &\sim F(y_i | \phi_i), \\
   \phi_i | G &\sim G(\phi_i), \\
   G | \alpha, H &\sim DP(\alpha,H).
\end{aligned}
\end{equation*}
Se dice que este es un \textit{modelo de mezclas} debido a que existen $y_i's$ que comparten un mismo valor para $\phi_i$ (por la propiedad discreta de $G$), y entonces estas $y_i's$ pueden ser consideradas pertenecientes a una misma subpoblaci\'on.

Es posible reescribir este modelo usando la equivalencia entre los Procesos de Dirichlet y el Proceso estoc\'astico de rompimiento de un palo, visto anteriormente. Sea $z_i$ el \textit{cluster} al que pertenece $y_i$ entre los $\Phi_1^*,\Phi_2^*,...$ posibles, se tiene entonces que $P(z_i = \Phi_k^*) = \pi_k$. Y si $\phi_k^*$ es el valor que comparten los miembros de $\Phi_k^*$, se usar\'a la notaci\'on $\phi_{z_i} = \phi_k^*$, cuando $z_i = \Phi_k^*$. Por lo tanto, el modelo se puede ahora escribir como
\begin{equation*}
\begin{aligned}
   y_i | z_i, \phi_k^* &\sim F(y_i | \phi_{z_i}), \\
   z_i | \pi &\sim Mult(\pi), \\
   \pi | \alpha &\sim GEM(\alpha), \\
   \phi_k^* | H &\sim H.
\end{aligned}
\end{equation*}

De esta manera, el Modelo de mezclas de Dirichlet es un modelo de mezclas infinitas, debido a que tiene un n\'umero infinito numerable de posibles \textit{clusters}, pero donde intuitivamente la importancia realmente recae s\'olo en aquellos que tienen un peso $\pi$ posterior mayor a cierto umbral, pero que son detectados hasta despu\'es de observar los datos; a diferencia de los modelos de mezclas finitas, que ya tienen un n\'umero de \textit{clusters} definidos previamente.

\newpage