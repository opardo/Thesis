\chapter[Paradigma bayesiano]{Paradigma bayesiano\raisebox{.3\baselineskip}{\normalsize\footnotemark}}\footnotetext{Las ideas de este cap\'itulo son retomadas de \cite{Denison_BayesMethods} y \cite{Bannerjee_BayLinMod}.}



\section{Axiomas}
Esta tesis da como aceptados los axiomas de la Estad\'istica Bayesiana, detallados durante muchos años en la literatura. Por ejemplo, pueden ser encontrados en \cite{Fishburn_Axioms}. Por lo tanto, entiende a dicho paradigma como el coherente para hacer estad\'istica, cuando una toma de decisi\'on con incertidumbre es el objetivo final del estudio. 

\section{Inferencia}

Un problema clásico de la estad\'istica es el de hacer predicci\'on, utilizando la informaci\'on de los datos que ya han sido observados. Por ejemplo, es posible pensar que ya se tiene el conjunto de $n$ datos observados $\{y_1, ..., y_n\}$ y se desea hacer predicci\'on acerca del valor del dato $y_{n+1}$, que a\'un no ha sido observado. Para esto, se podr\'ia usar la probabilidad condicional
\begin{equation*}
    p(y_{n+1}|y_1,...,y_n) =
    \frac{p(y_{n+1} \cap \{y_1, ..., y_n\})}{p(y_1, ..., y_n)} =
    \frac{p(y_1, ..., y_n,y_{n+1})}{p(y_1, ..., y_n)},
\end{equation*}
pero esto requerir\'ia conocer la funci\'on conjunta, misma que puede ser compleja por la estructura de dependencia de los datos.

No tiene mucho sentido suponer una estructura de independencia entre ellos, porque entonces el conjunto de observaciones $\{y_1, ..., y_n\}$ no dar\'ia informaci\'on alguna para $y_{n+1}$. Pero se puede suponer una distribuci\'on condicionalmente independiente. Es decir, se supone que cada una de las $y_i$'s tiene una misma distribuci\'on param\'etrica, con vector de par\'ametros $\theta$, y se cumple que
\begin{equation*}
    p(y_{k+1},y_k | \theta) =  p(y_k | \theta) \times p(y_{k+1} | \theta).
\end{equation*}

Siguiendo el mismo razonamiento, es posible obtener que
\begin{equation*}
    p(y_1,...,y_n | \theta) = \prod_{i=1}^n p(y_i|\theta). 
\end{equation*}

Dado que se desea hacer inferencia, y al igual que en otros paradigmas, se supone a $\theta$ como constante, pero desconocido. Una particularidad del paradigma bayesiano es expresar la incertidumbre que tiene el modelador acerca del valor verdadero mediante la asignaci\'on de una distribuci\'on a $\theta$, sujeta la informaci\'on inicial o conocimiento previo que se tenga del fen\'omeno $(H)$. Es decir, $p(\theta|H)$. Como una simplificaci\'on de la notaci\'on, en la literatura normalmente se escribe como $p(\theta) = p(\theta|H)$ y se conoce como la \textit{probabilidad inicial} del par\'ametro.

Regresando al problema inicial, y bajo los supuestos reci\'en mencionados, es importante notar que es posible escribir
\begin{equation*}
    p(y_{n+1} | y_1,...,y_n) =
    \int_{\Theta} 
    p(y_{n+1}|\theta) p(\theta|y_1,...y_n)d\theta,
\end{equation*}
donde a su vez, usando el \textbf{Teorema de Bayes}, se obtiene que
\begin{equation*}
\begin{aligned}
    p(\theta|y_1,...y_n) &=
    \frac{p(y_1,...y_n|\theta)p(\theta)}{p(y_1,...y_n)}\\ &=
    \frac{\left[\prod_{i=1}^n p(y_i|\theta)\right]p(\theta)}
    {\int_{\Theta}\left[\prod_{i=1}^n p(y_i|\theta)\right] p(\theta) d\theta},
\end{aligned}
\end{equation*}
que en el paradigma bayesiano se conoce como la \textit{probabilidad posterior} del par\'ametro.

Cabe observar que el denominador no depende de $\theta$, por lo que normalmente la probabilidad no se expresa como una igualdad, sino con la proporcionalidad
\begin{equation*}
    p(\theta|y_1,...,y_n) \propto p(y_1,...,y_n|\theta)p(\theta),
\end{equation*}
o en general,
\begin{equation*}
    Posterior \propto Verosimilitud \times Inicial.
\end{equation*}

Es importante notar que bajo este enfoque se obtiene una distribuci\'on completa para  el pron\'ostico de $y_{n+1}$. Esta se puede utilizar para el c\'alculo de estimaciones puntuales o intervalos, que en el caso del paradigma bayesiano son llamados de \textit{probabilidad}, mediante el uso de funciones de utilidad o p\'erdida, que son estudiadas con m\'as detalle en la Teor\'ia de Decisi\'on.

\section{Regresión lineal}

Se piensa un modelo de \textit{regresión a la media} tradicional, donde $y \in \mathbb{R}$ es la variable de respuesta y $x \in \mathbb{R}^n$ es el vector de covariables. La variable $\varepsilon \in \mathbb{R}$ representa el error aleatorio y se distribuye $\varepsilon \sim \mathcal{N}(0,\sigma^2)$, independiente respecto a $x$. De esta manera, entonces se tiene que:
\begin{equation*}
    y = \beta^Tx + \varepsilon \sim \mathcal{N}(\beta^T x,\sigma^2),
\end{equation*}
donde $\beta \in \mathbb{R}^n$ y $\sigma^2 \in \mathbb{R}^+$ se piensan con valores constantes, pero desconocidos, y la tarea es estimarlos.

Para hacer esto, el enfoque bayesiano le asigna una distribución de probabilidad a ambos par\'ametros, reflejando la incertidumbre que tiene el modelador acerca de su valor real. Es decir, sea $H$ la hip\'otesis o el conocimiento previo al que tiene acceso el modelador, se tiene que 
\begin{equation*}
    (\beta,\sigma^2) \sim P(\beta,\sigma^2|H).
\end{equation*}

A partir de este momento se omitir\'a escribir la distribuci\'on condicional respecto a $H$ por simplificaci\'on de la notaci\'on, pero es importante no olvidar su existencia.

Sea $\{(x_i,y_i)| x_i \in \mathbb{R}^n, y_i \in \mathbb{R}, i \in \{1,...,m\} \}$ el conjunto de datos observados, condicionalmente independientes e id\'enticamente distribuidos, de las variables de respuesta y de las covariables. Es posible representar este mismo conjunto con la notaci\'on matricial $\{X,Y | X \in \mathbb{R}^{m \times n}, Y \in \mathbb{R}^m\}$. Sea $\mathcal{E} \in \mathbb{R}^m$ el vector de errores aleatorios, tal que $\mathcal{E} \sim \mathcal{N}(0,\sigma^2 I)$. El modelo se puede reescribir como:
\begin{equation*}
    Y = X\beta + \mathcal{E} \sim \mathcal{N}(X\beta,\sigma^2 I).
\end{equation*}

Por el Teorema de Bayes,
\begin{equation*}
\begin{aligned}
    P(\beta,\sigma^2 | Y, X) 
    &= \frac{P(Y| X, \beta, \sigma^2) \times P(\beta, \sigma^2 | X)}{P(Y | X)} \\
    &= \frac{P(Y| X, \beta, \sigma^2) \times P(\beta, \sigma^2)}{P(Y | X)} \\
    &\propto P(Y| X, \beta, \sigma^2) \times P(\beta, \sigma^2), \\
\end{aligned}
\end{equation*}
donde $P(Y| X, \beta, \sigma^2)$ es la verosimilitud de los datos observados y se puede calcular como $P(Y| X, \beta, \sigma^2) = \mathcal{N}(X\beta,\sigma^2 I) = \prod_{i=1}^m \mathcal{N}(x_i^T\beta,\sigma^2)$. Por otro lado, $P(\beta,\sigma^2) = P(\beta,\sigma^2|H)$ es la distribuci\'on inicial de los par\'ametros.

Por conveniencia anl\'itica, hay una distribuci\'on inicial com\'unmente usada para los par\'ametros $\beta$ y $\sigma$ debido a que es conjugada respecto a la distribuci\'on Normal de los datos. Su nombre es \textit{Normal-Gamma Inversa (NGI)} y se dice que $\beta,\sigma^2 \sim \mathcal{NGI}(M,V,a,b)$, si
\begin{equation*}
\begin{aligned}
    P(\beta,\sigma^2) 
    &= P(\beta|\sigma^2) \times P(\sigma^2) \\
    &= \mathcal{N}(\beta|M, \sigma^2 V) \times \mathcal{GI}(\sigma ^2|a,b) \\
    &= \frac{1}{((2\pi)^n|\sigma^2 V|)^\frac{1}{2}}
       \exp\left(-\frac{1}{2}(\beta-M)^T{(\sigma^2 V)}^{-1}(\beta-M)\right) \\
    &  \text{ }\text{ }\text{ } \times
       \frac{b^a}{\Gamma(a)}(\sigma^2)^{-(a+1)}\exp\left(-\frac{b}{\sigma^2}\right) \\
    &= \frac{b^a}{(2\pi)^\frac{n}{2}{|V|}^\frac{1}{2}\Gamma(a)}(\sigma^2)^{-(a+    (n/2)+1)} \\
    & \text{ }\text{ }\text{ } \times
      \exp\left(-\frac{(\beta-M)^TV^{-1}(\beta-M) + 2b}{2\sigma^2}\right) \\
    &\propto (\sigma^2)^{-(a+(n/2)+1)} \exp\left(-\frac{(\beta-M)^TV^{-1}(\beta-M) + 2b}{2\sigma^2}\right),
\end{aligned}
\end{equation*}
donde $M$ es la media inicial de los coeficientes, $\sigma^2 V$ su varianza, y $a$ y $b$ son los par\'ametros iniciales de forma y medida de $\sigma ^2$. 

Aprovechando la propiedad conjugada, es posible escribir la probabilidad posterior de los par\'ametros como:
\begin{equation*}
\begin{aligned}
    P(\beta,\sigma^2 | Y, X) 
    &\propto P(Y| X, \beta, \sigma^2) \times P(\beta, \sigma^2), \\
    &\propto (\sigma^2)^{-(\bar{a}+(n/2)+1)} \exp\left(-\frac{(\beta-\bar{M})^T\bar{V}^{-1}(\beta-\bar{M}) + 2\bar{b}}{2\sigma^2}\right),
\end{aligned}
\end{equation*}
donde
\begin{equation*}
\begin{aligned}
    \bar{M} &= (V^{-1} + X^TX)^{-1} (V^{-1}M + X^TY), \\
    \bar{V} &= (V^{-1} + X^TX)^{-1}, \\
    \bar{a} &= a + n/2, \\
    \bar{b} &= b + \frac{\bar{M}^TV^{-1}M + Y^TY - \bar{M}^T\bar{V}^{-1}\bar{M}}{2}.
\end{aligned}
\end{equation*}

Es decir, la distribuci\'on posterior de $(\beta,\sigma^2)$ es \textit{Normal - Gamma Inversa}, con par\'ametros $\mathcal{NGI}(\bar{M},\bar{V},\bar{a},\bar{b})$.

Si se tiene una nueva matriz de covariables $X_*$ y se desea hacer predicci\'on de las respectivas variables de salida $Y_*$, es posible hacer inferencia con los datos observados de la siguiente manera:
\begin{equation*}
\begin{aligned}
    P(Y_*|X_*,Y,X)
    &= \int \int P(Y_*|X_*,\beta,\sigma^2) \times P(\beta,\sigma^2|Y,X) d\sigma^2 d\beta \\
    &= \int \int \mathcal{N}(X_*\beta,\sigma^2I) \times P(\beta,\sigma^2|Y,X) d\sigma^2 d\beta.
\end{aligned}
\end{equation*}

Particularmente, si se contin\'ua con el modelo conjugado \textit{Normal - Gamma Inversa / Normal}, es posible encontrar la soluci\'on anal\'itica:
\begin{equation*}
\begin{aligned}
    P(Y_*|X_*,Y,X)
    &= \int \int \mathcal{N}(X_*\beta,\sigma^2I) \times P(\beta,\sigma^2|Y,X) d\sigma^2 d\beta \\
    &= \int \int \mathcal{N}(X_*\beta,\sigma^2I) \times \mathcal{NGI}(\bar{M},\bar{V},\bar{a},\bar{b}) d\sigma^2 d\beta \\
    &= MVSt_{2\bar{a}} 
       \left(
        X_*\bar{M},\frac{\bar{b}}{\bar{a}}\left(I + X_*\bar{V}X_*^T\right)
       \right),
\end{aligned}
\end{equation*}
donde $MVSt$ es la distribuci\'on \textit{t-Student} multivariada, y cuya definici\'on se describe a continuaci\'on. 

\begin{defin}
    Sea $X \in \mathbb{R}^p$ un vector aleatorio, con media, mediana y moda $\mu$, matriz de covarianzas $\Sigma $, y $\nu$ grados de libertad, entonces $X \sim MVSt_{\nu}(\mu,\Sigma)$ si y s\'olo si su funci\'on de densidad es:
\begin{equation*}
    f(x|\mu,\sigma,\nu) = 
    \frac{\Gamma((\nu+p)/2)}{\Gamma(\nu/2)\nu^{p/2}\pi^{p/2}|\Sigma|^{1/2}}
    \left[1 + \frac{1}{\nu} (x-\mu)^T\Sigma^{-1}(x-\mu)\right]^{-\frac{\nu+p}{2}}.
\end{equation*}
\end{defin}

\newpage